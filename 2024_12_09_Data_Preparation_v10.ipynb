{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Executive Summary\n",
        "\n",
        "This Data Preparation notebook forms the first stage of a Retrieval-Augmented Generation (RAG) pipeline designed to process and index content from multiple document types (PDF, Word, PowerPoint, and Excel). The notebook locates files within a designated Google Drive folder, validates whether each has been previously processed, and reads any new documents for ingestion.\n",
        "\n",
        "Central to this pipeline is chunking: larger sections of text (especially from PDFs, Word headings, Excel rows, and PPT slides) are broken into manageable parts. Each chunk gets a unique document ID and includes important metadata, such as headings, slide numbers, row identifiers, or section titles. By splitting content into smaller pieces, the pipeline improves downstream retrieval accuracy and performance.\n",
        "\n",
        "Once chunked, the text is passed to the NebiusEmbedding class from LlamaIndex, which internally leverages Nebius-hosted embedding models. These embeddings transform text into high-dimensional vectors, capturing semantic meaning. They are then stored in two primary indexes:\n",
        "\n",
        "Whoosh Index (BM25-based) - supports text-based retrieval.\n",
        "Faiss Index (Vector-based) - supports semantic similarity searches.\n",
        "\n",
        "\n",
        "Overall, the Data Preparation notebook delivers a MVP that ingests documents, splits them into meaningful chunks, and creates indexes accessible to the second component of the pipeline. Logging and version tracking are also built in, ensuring traceability and easy debugging.\n",
        "\n",
        "OUTPUT FILES\n",
        "As the final result, this notebook creates files:\n",
        "FAISS\n",
        "1) default__vectore_store.faiss\n",
        "2) docstore.json\n",
        "3) graph_store.json\n",
        "4) index_store.json\n",
        "WHOOSH\n",
        "5) MAIN (Whoosh index)\n",
        "\n",
        "These files collectively contain vector data, text content, and metadata, allowing for seamless reloading in subsequent inference steps."
      ],
      "metadata": {
        "id": "TIat_nmUHp8t"
      },
      "id": "TIat_nmUHp8t"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backlog\n",
        "\n",
        "ISSUES TO ADDRESS:\n",
        "\n",
        "1) think of adding more metadata to pdf documents. Why don't keep and sections (if they exist) and pages for pdf documents?\n",
        "\n",
        "2) Need to systematize metadata. Now it looks very differently depending on file type:\n",
        "Entry 1:\n",
        "{\n",
        "    \"page_label\": \"1\",\n",
        "    \"file_name\": \"Test_file_pdf_(computer vision).pdf\"\n",
        "}\n",
        "\n",
        "or\n",
        "\n",
        "Entry 1:\n",
        "{\n",
        "    \"chunk_number\": 1,\n",
        "    \"total_chunks_in_section\": 1,\n",
        "    \"start_token\": 0,\n",
        "    \"end_token\": 529,\n",
        "    \"chunk_type\": \"token_chunk\",\n",
        "    \"source\": \"Autonomus vehicles and fusion.docx\"\n",
        "}\n",
        "\n",
        "3) add a count of tokens being used for embedding (Section 7)\n",
        "\n",
        "\n",
        "4) if we want to work with complicated excel and pdf files - we need to add LlamaParce functionality\n",
        "\n",
        "5) version control e.g. GIT isn't implemented\n",
        "\n",
        "6) regular saving and backup of crusial data, like, for example, faiss.index file"
      ],
      "metadata": {
        "id": "96I82tQ2kOzW"
      },
      "id": "96I82tQ2kOzW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1 Install required libraries"
      ],
      "metadata": {
        "id": "TaxyrO6peM7v"
      },
      "id": "TaxyrO6peM7v"
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Install required libraries\n",
        "!pip install pdfplumber pandas openpyxl bleach faiss-cpu openai\n",
        "!pip install -U llama-index llama-index-core llama-parse llama-index-readers-file openai\n",
        "!pip install llama-index-embeddings-nebius\n",
        "!pip install llama-index-vector-stores-faiss\n",
        "!pip install python-pptx python-docx\n",
        "!pip install whoosh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XC9obWiuNpHG",
        "outputId": "875be0b5-487d-4c3b-c2d1-448f02ca15bf"
      },
      "id": "XC9obWiuNpHG",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (6.2.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.9.0.post1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.60.2)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.11/dist-packages (0.12.14)\n",
            "Requirement already satisfied: llama-index-core in /usr/local/lib/python3.11/dist-packages (0.12.14)\n",
            "Requirement already satisfied: llama-parse in /usr/local/lib/python3.11/dist-packages (0.5.20)\n",
            "Requirement already satisfied: llama-index-readers-file in /usr/local/lib/python3.11/dist-packages (0.4.4)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.60.2)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.3)\n",
            "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.0)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.6.4)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.14)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.2)\n",
            "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.0)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.0)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (3.11.11)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (1.2.15)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (2.10.5)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (9.0.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (1.17.2)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.11/dist-packages (from llama-parse) (8.1.8)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file) (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file) (2.2.2)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file) (5.2.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file) (0.0.26)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file) (2.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core) (0.14.0)\n",
            "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.11)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core) (3.26.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file) (2025.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file) (1.17.0)\n",
            "Requirement already satisfied: llama-index-embeddings-nebius in /usr/local/lib/python3.11/dist-packages (0.3.1)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-nebius) (0.12.14)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-nebius) (0.3.1)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (3.11.11)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (1.2.15)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (2.10.5)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (9.0.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (1.17.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-openai<0.4.0,>=0.3.0->llama-index-embeddings-nebius) (1.60.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (1.18.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (2024.11.6)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.4.0,>=0.3.0->llama-index-embeddings-nebius) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.4.0,>=0.3.0->llama-index-embeddings-nebius) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.4.0,>=0.3.0->llama-index-embeddings-nebius) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.4.0,>=0.3.0->llama-index-embeddings-nebius) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (3.26.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (24.2)\n",
            "Requirement already satisfied: llama-index-vector-stores-faiss in /usr/local/lib/python3.11/dist-packages (0.3.0)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-vector-stores-faiss) (0.12.14)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (3.11.11)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.2.15)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2.10.5)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (9.0.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.17.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.18.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (3.26.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (0.14.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (24.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.3.1)\n",
            "Requirement already satisfied: python-pptx in /usr/local/lib/python3.11/dist-packages (1.0.2)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (11.1.0)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (3.2.1)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (4.12.2)\n",
            "Requirement already satisfied: whoosh in /usr/local/lib/python3.11/dist-packages (2.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2 Configuration: Mount Google Drive and Config Environment\n",
        "\n",
        "Mounts your Google Drive to access the project’s directories, configures environment variables (like the API key), and prepares important paths for reading and saving files. Ensures logging folders, data directories, and other essential locations exist. Also sets up the global Nebius endpoint (Nebius hosts lots of open-sources LLMs and has very reasonable per-token prices) and key for subsequent interactions."
      ],
      "metadata": {
        "id": "S-DEKP53dOTf"
      },
      "id": "S-DEKP53dOTf"
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Configuration: Mount Google Drive and config environment\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import logging\n",
        "import json\n",
        "import openai  # Import openai to set api_base globally\n",
        "\n",
        "# Logging configuration parameters\n",
        "logging_level = logging.INFO\n",
        "logging_format = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
        "log_folder=Path(\"/content/gdrive/MyDrive/RAG_Project_5/data_preparation/\")\n",
        "log_file=\"data_preparation.log\"\n",
        "max_bytes=5 * 1024 * 1024  # 5 MB\n",
        "backup_count=3  # Keep up to 3 backup files\n",
        "\n",
        "\n",
        "# Define directories for file processing and validate the directories exist\n",
        "data_directory = Path('/content/gdrive/MyDrive/RAG_Project_5/data')\n",
        "to_process_dir = data_directory / 'to_process'\n",
        "processed_dir = data_directory / 'processed'\n",
        "\n",
        "for folder in [to_process_dir, processed_dir]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)  # Create if not exists\n",
        "\n",
        "# Define directories for indexes\n",
        "whoosh_index_path = data_directory / \"whoosh_index\"\n",
        "os.makedirs(whoosh_index_path, exist_ok=True)\n",
        "\n",
        "faiss_index_path = data_directory / \"faiss_index\"\n",
        "os.makedirs(faiss_index_path, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "# Path to the JSON config file\n",
        "config_path = '/content/gdrive/MyDrive/Colab_Notebooks/config.json'\n",
        "\n",
        "# Load the API key (Nebius AI)\n",
        "with open(config_path, encoding='utf-8-sig') as config_file:\n",
        "    config = json.load(config_file)\n",
        "    os.environ['API_KEY'] = config['API_KEY']\n",
        "\n",
        "# Set the API key and API base globally\n",
        "openai.api_key = os.environ.get(\"API_KEY\")\n",
        "openai.api_base = \"https://api.studio.nebius.ai/v1/\"  # Nebius AI endpoint\n",
        "\n",
        "# Chunk parameters for word documents\n",
        "max_tokens=1024\n",
        "overlap_tokens=50\n",
        "\n",
        "# Define embedding model from Nebius AI Studio\n",
        "model_name = \"BAAI/bge-en-icl\" # from Nebius AI Studio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcoUPiDmN2By",
        "outputId": "2016588a-acb6-43b9-8347-046dc4adb1a8"
      },
      "id": "WcoUPiDmN2By",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Logging Configuration\n",
        "Sets up a rotating file handler to manage logs in both console and log files. Ensures logs don't grow indefinitely, helping maintain clarity during debugging or operational monitoring. This will record all activities in the data preparation process.\n"
      ],
      "metadata": {
        "id": "-iRWEvg6zVfL"
      },
      "id": "-iRWEvg6zVfL"
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Logging Configuration\n",
        "\n",
        "import logging\n",
        "from logging.handlers import RotatingFileHandler\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "def setup_logging(\n",
        "    log_folder: Path,\n",
        "    log_file: str = \"data_preparation.log\",\n",
        "    max_bytes: int = 5 * 1024 * 1024,  # 5 MB\n",
        "    backup_count: int = 3,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Configures logging with log rotation to write logs to both a file and the console.\n",
        "\n",
        "    Args:\n",
        "        log_folder (Path): The directory where the log file will be stored.\n",
        "        log_file (str, optional): The name of the log file. Defaults to \"data_preparation.log\".\n",
        "        max_bytes (int, optional): Maximum size of the log file in bytes before it is rotated. Defaults to 5 MB.\n",
        "        backup_count (int, optional): Number of backup files to keep (when log file exides max_bytes size, it becomes a backup file). Defaults to 3.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    os.makedirs(log_folder, exist_ok=True)  # Ensure the folder exists\n",
        "    log_file_path = log_folder / log_file\n",
        "\n",
        "    # Create a RotatingFileHandler\n",
        "    rotating_handler = RotatingFileHandler(\n",
        "        filename=log_file_path,\n",
        "        maxBytes=max_bytes,\n",
        "        backupCount=backup_count,\n",
        "        encoding=\"utf-8\",\n",
        "    )\n",
        "\n",
        "    # Configure logging\n",
        "    logging.basicConfig(\n",
        "        level = logging_level,\n",
        "        format = logging_format,\n",
        "        handlers=[\n",
        "            rotating_handler,  # Handles log rotation\n",
        "            logging.StreamHandler(),  # Logs also appear in Colab's output\n",
        "        ],\n",
        "        force=True,  # Force the configuration to apply even if logging was already configured\n",
        "    )\n",
        "\n",
        "    logging.info(\"Logging with rotation has been successfully configured.\")\n",
        "\n",
        "# Initialize logging with log rotation\n",
        "setup_logging(\n",
        "    log_folder=log_folder,\n",
        "    log_file=log_file,\n",
        "    max_bytes=max_bytes,\n",
        "    backup_count=backup_count,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bv1TSDxwPLUQ",
        "outputId": "8908a9eb-65ee-46f6-9c4c-7c7b0586e342"
      },
      "id": "bv1TSDxwPLUQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-02 10:40:08,659 - INFO - Logging with rotation has been successfully configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Load already processed files and validate new files for processing\n",
        "\n",
        "Loads existing records of processed files (from JSON) to avoid reprocessing duplicates. Scans the “to_process” directory to find any new PDFs, Excel files, PowerPoint decks, or Word documents. Logs and displays new files to process. This step helps keep track of which files have already been ingested into the indexing pipeline.\n",
        "\n",
        "Key output of the section: four lists of files (one for each file type) which are located in the Data directory, but haven't been processed yet: new_files_pdf, new_files_excel, etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "WP1GnDWqM0dh"
      },
      "id": "WP1GnDWqM0dh"
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Load already processed files and validate new files for processing\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Set, List\n",
        "\n",
        "def load_processed_files(record_path: Path) -> Set[str]:\n",
        "    \"\"\"\n",
        "    Loads the set of processed files from a JSON record.\n",
        "\n",
        "    Args:\n",
        "        record_path (Path): Path to the JSON file containing processed file names.\n",
        "\n",
        "    Returns:\n",
        "        Set[str]: A set of processed file names.\n",
        "        (In Python, a set is an unordered collection of unique item, no duplicates are allowed;\n",
        "        operations like adding, removing, and checking membership are efficient with sets)\n",
        "    \"\"\"\n",
        "    if record_path.exists():\n",
        "        with open(record_path, 'r') as f:\n",
        "            processed = set(json.load(f)) # converts the list into a set (checks and removes duplicates)\n",
        "        logging.info(f\"Loaded processed files from {record_path}.\")\n",
        "    else:\n",
        "        processed = set()\n",
        "        logging.info(f\"No processed files record found at {record_path}. Starting fresh.\")\n",
        "    return processed\n",
        "\n",
        "def log_new_files(file_list: List[Path], file_type: str) -> None:\n",
        "    \"\"\"\n",
        "    Logs and prints a summary of new files to process.\n",
        "\n",
        "    Args:\n",
        "        file_list (List[Path]): List of new files to process.\n",
        "        file_type (str): The type of files (e.g., 'PDF', 'Excel', 'PPT', 'DOC').\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    if not file_list:\n",
        "        logging.info(f\"No new {file_type.upper()} files found in {to_process_dir}.\")\n",
        "        print(f\"No new {file_type.upper()} files to process.\\n\")\n",
        "    else:\n",
        "        logging.info(f\"Found {len(file_list)} new {file_type.upper()} files to process.\")\n",
        "        print(f\"New {file_type.upper()} Files Found:\")\n",
        "        for file in file_list:\n",
        "            print(file)\n",
        "        print()\n",
        "\n",
        "# Define paths for records of processed files per file type\n",
        "processed_files_record_pdf = processed_dir / \"processed_files_pdf.json\"\n",
        "processed_files_record_excel = processed_dir / \"processed_files_excel.json\"\n",
        "processed_files_record_ppt = processed_dir / \"processed_files_ppt.json\"\n",
        "processed_files_record_doc = processed_dir / \"processed_files_doc.json\"\n",
        "\n",
        "# Load processed files\n",
        "processed_files_pdf = load_processed_files(processed_files_record_pdf)\n",
        "processed_files_excel = load_processed_files(processed_files_record_excel)\n",
        "processed_files_ppt = load_processed_files(processed_files_record_ppt)\n",
        "processed_files_doc = load_processed_files(processed_files_record_doc)\n",
        "\n",
        "\n",
        "# Supported extensions for processing\n",
        "supported_extensions = [\".pdf\", \".xls\", \".xlsx\", \".ppt\", \".pptx\", \".doc\", \".docx\"]\n",
        "\n",
        "# List all relevant files recursively from nested folders\n",
        "all_files = [f for f in to_process_dir.rglob(\"*\") if f.is_file() and f.suffix.lower() in supported_extensions]\n",
        "\n",
        "# Categorize files by type\n",
        "files_pdf = [f for f in all_files if f.suffix.lower() == \".pdf\"]\n",
        "files_excel = [f for f in all_files if f.suffix.lower() in [\".xls\", \".xlsx\"]]\n",
        "#files_ppt = [f for f in all_files if f.suffix.lower() == \".ppt\"]\n",
        "files_ppt = [f for f in all_files if f.suffix.lower() in [\".ppt\", \".pptx\"]]\n",
        "files_doc = [f for f in all_files if f.suffix.lower() in [\".doc\", \".docx\"]]\n",
        "\n",
        "# Identify new files for processing\n",
        "new_files_pdf = [f for f in files_pdf if f.name not in processed_files_pdf]\n",
        "new_files_excel = [f for f in files_excel if f.name not in processed_files_excel]\n",
        "new_files_ppt = [f for f in files_ppt if f.name not in processed_files_ppt]\n",
        "new_files_doc = [f for f in files_doc if f.name not in processed_files_doc]\n",
        "\n",
        "# Validate new files\n",
        "log_new_files(new_files_pdf, \"PDF\")\n",
        "log_new_files(new_files_excel, \"Excel\")\n",
        "log_new_files(new_files_ppt, \"PPT\")\n",
        "log_new_files(new_files_doc, \"DOC\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "okQ4_As9pZ1-",
        "outputId": "3bbce9bb-2319-4f6c-b2d8-cc5883981f4a",
        "collapsed": true
      },
      "id": "okQ4_As9pZ1-",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'processed_dir' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-eb724852df29>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# Define paths for records of processed files per file type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mprocessed_files_record_pdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessed_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"processed_files_pdf.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0mprocessed_files_record_excel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessed_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"processed_files_excel.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mprocessed_files_record_ppt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessed_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"processed_files_ppt.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'processed_dir' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5 Setup of utility functions for Data Ingestion & Preprocessing Sections\n",
        "\n",
        "Provides helper functions used throughout the data ingestion and preprocessing pipeline. It includes functions to assign unique IDs to documents and enrich them with metadata like source filename or other relevant fields."
      ],
      "metadata": {
        "id": "hnf-4xGvNqo8"
      },
      "id": "hnf-4xGvNqo8"
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 Setup of utility functions for Data Ingestion & Preprocessing Sections\n",
        "\n",
        "from llama_index.core import Document\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "import uuid\n",
        "\n",
        "# Functions are used in preprocessing PDF and WORD files\n",
        "def assign_doc_id(documents: list) -> list:\n",
        "    \"\"\"\n",
        "    Assigns a unique doc_id to each document, adds this doc_id to metadata.\n",
        "\n",
        "    Args:\n",
        "        documents (list): List of Document objects.\n",
        "\n",
        "    Returns:\n",
        "        list: List of Document objects with assigned doc_id.\n",
        "    \"\"\"\n",
        "    for doc in documents:\n",
        "        # Assign a unique ID\n",
        "        doc_id = str(uuid.uuid4())\n",
        "        doc.metadata[\"doc_id\"] = doc_id\n",
        "    return documents\n",
        "\n",
        "def extract_metadata(doc: Document, source_file: str, additional_info: dict) -> Document:\n",
        "    \"\"\"\n",
        "    Updates Document metadata with additional information: source file name and other key-value pairs from additional info.\n",
        "\n",
        "    Args:\n",
        "        doc (Document): The document object.\n",
        "        source_file (str): The source file name.\n",
        "        additional_info (dict): Additional metadata information.\n",
        "\n",
        "    Returns:\n",
        "        Document: Document object with updated metadata.\n",
        "    \"\"\"\n",
        "    doc.metadata[\"source\"] = source_file\n",
        "    for key, value in additional_info.items():\n",
        "        doc.metadata[key] = value\n",
        "    return doc"
      ],
      "metadata": {
        "id": "TIGIRRyPjo_2"
      },
      "id": "TIGIRRyPjo_2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Data Ingestion & Preprocessing for PDF\n",
        "\n",
        "Loads PDF files using LlamaIndex's PDFReader, converting each page into a separate Document instance. Automatically assigns document IDs and attaches metadata (like the source filename). Collects all pages from the provided list of new PDFs, logs any processing errors, and returns a list of documents ready for indexing.\n",
        "\n",
        "Key output of the section - list of Document objects, each representing a page of a pdf file from new_files_pdf list of files (see Section 4)\n",
        "\n"
      ],
      "metadata": {
        "id": "Oq-7DsIVlw9J"
      },
      "id": "Oq-7DsIVlw9J"
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Data Ingestion & Preprocessing for PDF\n",
        "\n",
        "from llama_index.core import Document\n",
        "from llama_index.readers.file import PDFReader\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "# import uuid\n",
        "\n",
        "def load_pdf_docs(pdf_paths: List[Path]) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Uses LlamaIndex PDFReader to load text content of PDF files\n",
        "    and organizes them into Document objects.\n",
        "\n",
        "    Each page of a PDF file (with associated metadata) will be a separate Document.\n",
        "\n",
        "    Args:\n",
        "        pdf_paths (List[Path]): List of PDF file paths to process.\n",
        "\n",
        "    Returns:\n",
        "        List[Document]: A list of Document objects containing the text and metadata from the PDFs.\n",
        "    \"\"\"\n",
        "    pdf_reader = PDFReader()\n",
        "    all_docs = []\n",
        "    for pdf_path in pdf_paths:\n",
        "        try:\n",
        "            docs = pdf_reader.load_data(pdf_path)\n",
        "            docs = assign_doc_id(docs)  # Assign doc_id and add doc_id to metadata\n",
        "            # Extract additional metadata if available\n",
        "            for doc in docs:\n",
        "                additional_info = {\n",
        "                  #  \"page_number\": doc.metadata.get(\"page_number\"), # no need for page number here - \"page label\" will be extracted automatically\n",
        "                    # Add other metadata fields as necessary\n",
        "                }\n",
        "                doc = extract_metadata(doc, source_file=pdf_path.name, additional_info=additional_info)\n",
        "            all_docs.extend(docs)\n",
        "            logging.info(f\"Processed PDF: {pdf_path.name} with {len(docs)} pages.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to process PDF {pdf_path.name}: {e}\")\n",
        "    return all_docs\n",
        "\n",
        "# Load pdf docs\n",
        "if new_files_pdf:\n",
        "    pdf_docs = load_pdf_docs(new_files_pdf)\n",
        "    logging.info(f\"Total new PDF documents loaded: {len(pdf_docs)}\")\n",
        "else:\n",
        "    pdf_docs = []\n",
        "    logging.info(\"No new PDF files to process.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvQhR0p_ANfD",
        "outputId": "455c9038-2d9f-4719-bcc5-2830aa678262"
      },
      "id": "UvQhR0p_ANfD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-02 11:34:11,366 - INFO - No new PDF files to process.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Data Ingestion & Preprocessing for Excel\n",
        "\n",
        "Parses Excel files sheet by sheet and row by row, converting each row (+ concatenated headers) into a separate Document. Uses Pandas to read each sheet, constructs a text string describing each row's columns and data, and attaches metadata like sheet names and row numbers.\n",
        "\n",
        "Key output of the section - list of Document objects, each representing a row of an excel file from new_files_excel set of files (see Section 4)"
      ],
      "metadata": {
        "id": "L_0ahknZ0S04"
      },
      "id": "L_0ahknZ0S04"
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Data Ingestion & Preprocessing for Excel\n",
        "\n",
        "from llama_index.core import Document\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "def load_excel_docs(excel_paths: List[Path]) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Uses Pandas to load documents from Excel files and organizes them into Document objects.\n",
        "\n",
        "    Each row of an Excel sheet is converted into a Document with metadata.\n",
        "\n",
        "    Args:\n",
        "        excel_paths (List[Path]): List of Excel file paths to process.\n",
        "\n",
        "    Returns:\n",
        "        List[Document]: A list of Document objects containing the text and metadata from the Excel files.\n",
        "    \"\"\"\n",
        "    all_docs = []\n",
        "    for excel_path in excel_paths:\n",
        "        try:\n",
        "            xls = pd.ExcelFile(excel_path)\n",
        "\n",
        "            for sheet_name in xls.sheet_names:\n",
        "                df = pd.read_excel(xls, sheet_name=sheet_name)\n",
        "\n",
        "                for idx, row in df.iterrows():\n",
        "                    row_dict = \", \".join(f\"{col.strip().lower()}: {str(row[col]).strip()}\" for col in df.columns)\n",
        "                    metadata = {\n",
        "                        \"sheet\": sheet_name,\n",
        "                        \"row_number\": idx + 1,  # 1-based indexing\n",
        "                    }\n",
        "\n",
        "                    doc = Document(text=row_dict, metadata=metadata)\n",
        "                    all_docs.append(doc)\n",
        "\n",
        "            logging.info(f\"Processed Excel: {excel_path.name} with {len(df)} rows.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to process Excel {excel_path.name}: {e}\")\n",
        "\n",
        "    # Assign doc_ids and extract metadata\n",
        "    all_docs = assign_doc_id(all_docs)\n",
        "    for doc in all_docs:\n",
        "        additional_info = {}  # Add any additional metadata if necessary\n",
        "        doc = extract_metadata(doc, source_file=excel_path.name, additional_info=additional_info)\n",
        "    return all_docs\n",
        "\n",
        "if new_files_excel:\n",
        "    excel_docs = load_excel_docs(new_files_excel)\n",
        "    logging.info(f\"Total new Excel documents loaded: {len(excel_docs)}\")\n",
        "else:\n",
        "    excel_docs = []\n",
        "    logging.info(\"No new Excel files to process.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLiUaF1Se0C0",
        "outputId": "29ec4fec-e1de-4977-fbe1-7b768f18b250"
      },
      "id": "sLiUaF1Se0C0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-02 11:34:15,238 - INFO - Processed Excel: Lessons Learned_2021 BOHAI_ZPEC Campaign_MD_short.xlsx with 49 rows.\n",
            "2025-01-02 11:34:15,241 - INFO - Total new Excel documents loaded: 49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Data Ingestion & Preprocessing for PowerPoint\n",
        "\n",
        "Extracts text content slide by slide from PowerPoint presentations. Each slide's text gets packaged into a Document with metadata (slide number, source file). This approach ensures each slide is indexed separately, making it easier to search for content within individual slides later on."
      ],
      "metadata": {
        "id": "ygY0zstnRn-l"
      },
      "id": "ygY0zstnRn-l"
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Data Ingestion & Preprocessing for PowerPoint\n",
        "\n",
        "from pptx import Presentation\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "from llama_index.core import Document\n",
        "\n",
        "def load_ppt_docs(ppt_paths: List[Path]) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Loads text content from PowerPoint files and organizes them into Document objects.\n",
        "\n",
        "    Each slide is converted into a Document with metadata.\n",
        "\n",
        "    Args:\n",
        "        ppt_paths (List[Path]): List of PowerPoint file paths to process.\n",
        "\n",
        "    Returns:\n",
        "        List[Document]: A list of Document objects containing the text and metadata from the slides.\n",
        "    \"\"\"\n",
        "    all_docs = []\n",
        "    for ppt_path in ppt_paths:\n",
        "        try:\n",
        "            prs = Presentation(ppt_path)\n",
        "            for idx, slide in enumerate(prs.slides):\n",
        "                slide_text = \"\\n\".join(shape.text for shape in slide.shapes if hasattr(shape, \"text\"))  # Collects only text from the slide\n",
        "                metadata = {\n",
        "                    \"slide_number\": idx + 1,\n",
        "                }\n",
        "                doc = Document(text=slide_text, metadata=metadata)\n",
        "                all_docs.append(doc)\n",
        "            logging.info(f\"Processed PowerPoint: {ppt_path.name} with {len(prs.slides)} slides.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to process PowerPoint {ppt_path.name}: {e}\")\n",
        "\n",
        "    # Assign doc_ids and extract metadata\n",
        "    all_docs = assign_doc_id(all_docs)\n",
        "    for doc in all_docs:\n",
        "        additional_info = {}  # Add any additional metadata if necessary\n",
        "        doc = extract_metadata(doc, source_file=ppt_path.name, additional_info=additional_info)\n",
        "    return all_docs\n",
        "\n",
        "if new_files_ppt:\n",
        "    ppt_docs = load_ppt_docs(new_files_ppt)\n",
        "    logging.info(f\"Total new PowerPoint documents loaded: {len(ppt_docs)}\")\n",
        "else:\n",
        "    ppt_docs = []\n",
        "    logging.info(\"No new PowerPoint files to process.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw99vJaKhdAV",
        "outputId": "dd3fddd1-c4f5-4782-aaa0-fa45719c9ad2"
      },
      "id": "rw99vJaKhdAV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-02 11:34:19,616 - INFO - No new PowerPoint files to process.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. Data Ingestion & Preprocessing for Word\n",
        "\n",
        "Loads Word documents, splitting them into sections based on headings. Large sections are further subdivided into smaller chunks by token count to ensure more granular indexing. Each chunk gets its own metadata, including heading titles, chunk numbers, and a unique document ID."
      ],
      "metadata": {
        "id": "Wl94u2J_RrVP"
      },
      "id": "Wl94u2J_RrVP"
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Data Ingestion & Preprocessing for Word\n",
        "\n",
        "# !!! I plan to rewrite this section to use assing_doc_id and extract_metadata utility functions to make my code more smooth and readable.\n",
        "\n",
        "from docx import Document as WordDocument\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "from llama_index.core import Document\n",
        "from transformers import GPT2TokenizerFast\n",
        "import uuid\n",
        "import textwrap\n",
        "\n",
        "# Initialize tokenizer for token-based chunking\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "\n",
        "def split_large_section(\n",
        "    section: List[str], metadata: dict, max_tokens: int, overlap_tokens: int\n",
        ") -> List[Document]:\n",
        "    \"\"\"\n",
        "    Splits a large text section into smaller chunks based on token count.\n",
        "\n",
        "    Args:\n",
        "        section (List[str]): List of paragraphs in the section.\n",
        "        metadata (dict): Metadata associated with the section.\n",
        "        max_tokens (int): Maximum number of tokens per chunk.\n",
        "        overlap_tokens (int): Number of overlapping tokens between chunks.\n",
        "\n",
        "    Returns:\n",
        "        List[Document]: List of smaller Document objects.\n",
        "    \"\"\"\n",
        "    all_docs = []\n",
        "    text = \"\\n\".join(section)\n",
        "    tokens = tokenizer.encode(text)\n",
        "    total_chunks = (len(tokens) + max_tokens - 1) // max_tokens  # Calculate total chunks\n",
        "\n",
        "    for chunk_number, i in enumerate(range(0, len(tokens), max_tokens - overlap_tokens), start=1):\n",
        "        chunk_tokens = tokens[i : i + max_tokens]\n",
        "        chunk_text = tokenizer.decode(chunk_tokens)\n",
        "\n",
        "        # Add metadata for the current chunk\n",
        "        chunk_metadata = {\n",
        "            **(metadata or {}),\n",
        "            \"chunk_number\": chunk_number,\n",
        "            \"total_chunks_in_section\": total_chunks,\n",
        "            \"start_token\": i,\n",
        "            \"end_token\": min(i + max_tokens, len(tokens)),\n",
        "            \"chunk_type\": \"token_chunk\",\n",
        "            \"doc_id\": str(uuid.uuid4()),  # Assign unique doc_id for each chunk\n",
        "        }\n",
        "\n",
        "        doc = Document(text=chunk_text, metadata=chunk_metadata)\n",
        "        all_docs.append(doc)\n",
        "\n",
        "    return all_docs\n",
        "\n",
        "def chunk_by_headings(\n",
        "    word_doc: WordDocument, max_tokens: int = 1024, overlap_tokens: int = 50\n",
        ") -> List[Document]:\n",
        "    \"\"\"\n",
        "    Chunks a Word document into sections based on headings and splits large sections by tokens.\n",
        "\n",
        "    Args:\n",
        "        word_doc (WordDocument): The Word document object.\n",
        "        max_tokens (int, optional): Maximum number of tokens per chunk. Defaults to 1024.\n",
        "        overlap_tokens (int, optional): Number of overlapping tokens between chunks. Defaults to 50.\n",
        "\n",
        "    Returns:\n",
        "        List[Document]: List of Document objects containing text and metadata.\n",
        "    \"\"\"\n",
        "    all_docs = []\n",
        "    current_section = []\n",
        "    current_metadata = None\n",
        "\n",
        "    for paragraph in word_doc.paragraphs:\n",
        "        if paragraph.style.name.startswith(\"Heading\"):  # Detect headings\n",
        "            # Save the current section as chunks\n",
        "            if current_section:\n",
        "                all_docs.extend(\n",
        "                    split_large_section(\n",
        "                        current_section, current_metadata, max_tokens, overlap_tokens\n",
        "                    )\n",
        "                )\n",
        "                current_section = []\n",
        "\n",
        "            # Update metadata for the new section\n",
        "            current_metadata = {\"section\": paragraph.text.strip()}\n",
        "\n",
        "        # Add paragraph text to the current section\n",
        "        current_section.append(paragraph.text.strip())\n",
        "\n",
        "    # Save the last section if it contains any content\n",
        "    if current_section:\n",
        "        all_docs.extend(\n",
        "            split_large_section(\n",
        "                current_section, current_metadata, max_tokens, overlap_tokens\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return all_docs\n",
        "\n",
        "def load_doc_docs(\n",
        "    doc_paths: List[Path], max_tokens: int = 1024, overlap_tokens: int = 50\n",
        ") -> List[Document]:\n",
        "    \"\"\"\n",
        "    Processes Word documents by chunking them into sections based on headings.\n",
        "\n",
        "    Args:\n",
        "        doc_paths (List[Path]): List of Word document file paths to process.\n",
        "        max_tokens (int): Maximum number of tokens per chunk.\n",
        "        overlap_tokens (int): Number of overlapping tokens between chunks.\n",
        "\n",
        "    Returns:\n",
        "        List[Document]: List of Document objects containing the text and metadata from the Word files.\n",
        "    \"\"\"\n",
        "    all_docs = []\n",
        "    for doc_path in doc_paths:\n",
        "        try:\n",
        "            word_doc = WordDocument(doc_path)\n",
        "            logging.info(f\"Processing {doc_path.name} by headings.\")\n",
        "\n",
        "            # Use chunk_by_headings function\n",
        "            docs = chunk_by_headings(word_doc, max_tokens, overlap_tokens)\n",
        "\n",
        "            # Ensure all chunks include the source metadata\n",
        "            for doc in docs:\n",
        "                if 'source' not in doc.metadata:\n",
        "                    doc.metadata['source'] = doc_path.name  # Add source metadata if missing\n",
        "\n",
        "            all_docs.extend(docs)\n",
        "            logging.info(f\"Processed Word Document: {doc_path.name} with {len(docs)} chunks.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to process Word Document {doc_path.name}: {e}\")\n",
        "    return all_docs\n",
        "\n",
        "if new_files_doc:\n",
        "    doc_docs = load_doc_docs(new_files_doc, max_tokens=max_tokens, overlap_tokens=overlap_tokens)\n",
        "    logging.info(f\"Total new Word documents loaded: {len(doc_docs)}\")\n",
        "else:\n",
        "    doc_docs = []\n",
        "    logging.info(\"No new Word files to process.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDLEqNgJDTBW",
        "outputId": "cb51e187-1e4a-4aaa-d0ec-1b29d744f26e"
      },
      "id": "QDLEqNgJDTBW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-02 11:34:23,612 - INFO - No new Word files to process.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10 Create / Update Whoosh Index - add new documents to it\n",
        "\n",
        "Creates or opens an existing Whoosh index to store newly ingested documents. Indexes fields such as text content, source, and any additional metadata like sheet numbers, slide numbers, or section headings. This step updates the search index with new documents so they can be retrieved later using BM25 or other text-based query methods."
      ],
      "metadata": {
        "id": "lt-T3qv4qvOv"
      },
      "id": "lt-T3qv4qvOv"
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Create / Update Whoosh Index - add new documents to it\n",
        "\n",
        "from whoosh.index import create_in, open_dir, exists_in\n",
        "from whoosh.fields import Schema, TEXT, ID, NUMERIC\n",
        "from whoosh import index\n",
        "from pathlib import Path\n",
        "import os\n",
        "import logging\n",
        "import uuid\n",
        "\n",
        "# Define the schema with all necessary metadata fields\n",
        "schema = Schema(\n",
        "    content=TEXT(stored=True),                  # Store the text content for search\n",
        "    source=ID(stored=True),                     # Store the document source as an identifier\n",
        "    sheet=ID(stored=True),                      # For Excel documents\n",
        "    row_number=NUMERIC(stored=True, sortable=True),\n",
        "    slide_number=NUMERIC(stored=True, sortable=True),  # For PPT documents\n",
        "    section=TEXT(stored=True),                  # For Word documents\n",
        "    chunk_number=NUMERIC(stored=True, sortable=True),\n",
        "    total_chunks_in_section=NUMERIC(stored=True, sortable=True),\n",
        "    page_number=NUMERIC(stored=True, sortable=True),    # For PDF documents\n",
        "    doc_id=ID(stored=True, unique=True),         # Unique identifier for each document\n",
        "    # Add other metadata fields as needed\n",
        ")\n",
        "\n",
        "def create_or_update_whoosh_index(documents: list, index_dir: Path) -> None:\n",
        "    \"\"\"\n",
        "    Creates or updates a Whoosh index with the given documents, including all metadata fields.\n",
        "\n",
        "    Args:\n",
        "        documents (list): List of Document objects to add to the index.\n",
        "        index_dir (Path): Directory where the Whoosh index will be stored.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Ensure the index directory exists\n",
        "        os.makedirs(index_dir, exist_ok=True)\n",
        "\n",
        "        # Check if an existing Whoosh index exists or create a new one\n",
        "        if not exists_in(index_dir):\n",
        "            # Create a new Whoosh index\n",
        "            idx = create_in(index_dir, schema)\n",
        "            logging.info(f\"Created a new Whoosh index at {index_dir}.\")\n",
        "            before_count = 0  # No documents in a newly created index\n",
        "        else:\n",
        "            # Open the existing Whoosh index\n",
        "            idx = open_dir(index_dir)\n",
        "            with idx.searcher() as searcher:\n",
        "                before_count = searcher.doc_count()  # Count existing documents\n",
        "            logging.info(f\"Opened existing Whoosh index at {index_dir}. Total docs before: {before_count}.\")\n",
        "\n",
        "        # Add or update documents in the index\n",
        "        writer = idx.writer()\n",
        "\n",
        "        for doc in documents:\n",
        "            try:\n",
        "                # Extract content and source from the document\n",
        "                content = doc.text\n",
        "                source = doc.metadata.get(\"source\", \"unknown_source\")\n",
        "                doc_id = doc.metadata.get(\"doc_id\", str(uuid.uuid4()))  # Ensure doc_id exists\n",
        "\n",
        "                # Prepare metadata fields\n",
        "                doc_fields = {\n",
        "                    \"content\": content,\n",
        "                    \"source\": source,\n",
        "                    \"doc_id\": doc_id,\n",
        "                }\n",
        "\n",
        "                # Conditionally add metadata fields if they exist\n",
        "                if \"sheet\" in doc.metadata:\n",
        "                    doc_fields[\"sheet\"] = doc.metadata[\"sheet\"]\n",
        "                if \"row_number\" in doc.metadata:\n",
        "                    doc_fields[\"row_number\"] = doc.metadata[\"row_number\"]\n",
        "                if \"slide_number\" in doc.metadata:\n",
        "                    doc_fields[\"slide_number\"] = doc.metadata[\"slide_number\"]\n",
        "                if \"section\" in doc.metadata:\n",
        "                    doc_fields[\"section\"] = doc.metadata[\"section\"]\n",
        "                if \"chunk_number\" in doc.metadata:\n",
        "                    doc_fields[\"chunk_number\"] = doc.metadata[\"chunk_number\"]\n",
        "                if \"total_chunks_in_section\" in doc.metadata:\n",
        "                    doc_fields[\"total_chunks_in_section\"] = doc.metadata[\"total_chunks_in_section\"]\n",
        "                if \"page_number\" in doc.metadata:\n",
        "                    doc_fields[\"page_number\"] = doc.metadata[\"page_number\"]\n",
        "\n",
        "                # Add or update document in the index\n",
        "                writer.update_document(**doc_fields)\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Failed to index document {doc.metadata.get('doc_id', 'Unknown ID')}: {e}\")\n",
        "\n",
        "        # Commit changes to the index\n",
        "        writer.commit()\n",
        "\n",
        "        # Count documents after updating the index\n",
        "        with idx.searcher() as searcher:\n",
        "            after_count = searcher.doc_count()\n",
        "\n",
        "        # Log the document counts\n",
        "        newly_added = len(documents)\n",
        "        logging.info(\n",
        "            f\"Whoosh index updated. Docs before: {before_count}, docs added: {newly_added}, docs after: {after_count}.\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to create/update Whoosh index at {index_dir}: {e}\")\n",
        "        raise\n",
        "\n",
        "# We don't use backup_whoosh_index explicitly yet\n",
        "def backup_whoosh_index(whoosh_index_path: Path, backup_path: Path) -> None:\n",
        "    \"\"\"\n",
        "    Creates a backup of the Whoosh index.\n",
        "\n",
        "    Args:\n",
        "        whoosh_index_path (Path): Path to the Whoosh index.\n",
        "        backup_path (Path): Path to store the backup.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    import shutil  # Imported here as per coding preference\n",
        "\n",
        "    try:\n",
        "        if whoosh_index_path.exists():\n",
        "            shutil.copytree(whoosh_index_path, backup_path, dirs_exist_ok=True)\n",
        "            logging.info(f\"Whoosh index backed up to {backup_path}.\")\n",
        "        else:\n",
        "            logging.warning(f\"Whoosh index path {whoosh_index_path} does not exist. Backup skipped.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to backup Whoosh index: {e}\")\n",
        "\n",
        "# Define all_new_docs by aggregating all processed documents\n",
        "all_new_docs = pdf_docs + excel_docs + ppt_docs + doc_docs\n",
        "\n",
        "# Execute indexing\n",
        "# whoosh_index_path = data_directory / \"whoosh_index\" - moved to config section\n",
        "create_or_update_whoosh_index(all_new_docs, whoosh_index_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuLbT-bkl-Z-",
        "outputId": "af08838d-6df3-4352-a0c2-d35584530ff2"
      },
      "id": "vuLbT-bkl-Z-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-02 11:34:29,125 - INFO - Created a new Whoosh index at /content/gdrive/MyDrive/RAG_Project_5/data/whoosh_index.\n",
            "2025-01-02 11:34:29,483 - INFO - Whoosh index updated. Docs before: 0, docs added: 49, docs after: 49.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Create / Update Faiss Index - add new documents to it\n",
        "\n",
        "Builds or updates a Faiss-based vector index for semantic search. Uses the Nebius embedding model to create embeddings for all new documents, then stores these vectors in a Faiss index. If an index doesn't exist, it is created from scratch; otherwise, it is loaded and incrementally updated with any newly ingested documents.\n",
        "\n",
        "Key output of the section - updated vector database (Index) - vectorstore. Vectorstore contains text embeddings, metadata and text chunks of processed files."
      ],
      "metadata": {
        "id": "1RoycV940aHe"
      },
      "id": "1RoycV940aHe"
    },
    {
      "cell_type": "code",
      "source": [
        "# 11 Create / Update Faiss Index - add new documents to it\n",
        "\n",
        "import faiss\n",
        "import httpx\n",
        "from pathlib import Path\n",
        "from llama_index.embeddings.nebius import NebiusEmbedding\n",
        "from llama_index.vector_stores.faiss import FaissVectorStore\n",
        "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage\n",
        "from llama_index.core.storage.docstore.simple_docstore import SimpleDocumentStore\n",
        "from llama_index.core.storage.index_store import SimpleIndexStore\n",
        "from llama_index.core import Document, Settings\n",
        "\n",
        "import uuid # do we really need it here?\n",
        "import logging\n",
        "\n",
        "# Setup NebiusEmbedding model with custom_http_client\n",
        "# model_name = \"BAAI/bge-en-icl\" #- setup in config section\n",
        "custom_http_client = httpx.Client(timeout=60.0)\n",
        "embedding_model = NebiusEmbedding(\n",
        "    api_key=os.environ.get(\"API_KEY\"),\n",
        "    model_name=model_name,\n",
        "    http_client=custom_http_client,\n",
        "    api_base=\"https://api.studio.nebius.ai/v1/\",  # Explicitly specify api_base\n",
        "    # api_base=api_base,  # Using api_base variable defined in the config section doesn't work - probably, due to some bugs in NebiusEmbedding module\n",
        ")\n",
        "\n",
        "# Determine dimension of embedding model - needed for FAISS index\n",
        "sample_text = \"test text to create an embedding\"\n",
        "sample_embedding = embedding_model.get_text_embedding(sample_text)\n",
        "embedding_dimension = len(sample_embedding)\n",
        "\n",
        "print(f\"\\nEmbedding Model {model_name} creates {embedding_dimension}-dimensional vectors.\\n\")\n",
        "print(f\"Sample embedding for text '{sample_text}':\")\n",
        "print(sample_embedding[:10], \"...\\n\")\n",
        "\n",
        "# Set the embedding model in LlamaIndex settings\n",
        "Settings.embed_model = embedding_model\n",
        "\n",
        "def create_or_update_faiss_index(\n",
        "    documents: list[Document], index_path: Path, embedding_model: NebiusEmbedding, embedding_dimension: int\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Creates or updates a FAISS-based index with the given documents.\n",
        "\n",
        "    Args:\n",
        "        documents (list[Document]): List of Document objects to add to the index.\n",
        "        index_path (Path): Path to the directory where the index will be stored.\n",
        "        embedding_model (NebiusEmbedding): The embedding model used for indexing.\n",
        "        embedding_dimension (int): The dimension of the embeddings.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    if documents:\n",
        "        newly_added = len(documents)\n",
        "        if (index_path / \"default__vector_store.faiss\").exists():\n",
        "            # Load existing index\n",
        "            vector_store = FaissVectorStore.from_persist_path(str(index_path / \"default__vector_store.faiss\"))\n",
        "            storage_context = StorageContext.from_defaults(vector_store=vector_store, persist_dir=str(index_path))  # Reconstruct storage_context from files\n",
        "            index = load_index_from_storage(storage_context, embedding=embedding_model)  # Reloading the index\n",
        "\n",
        "            # Count vectors BEFORE\n",
        "            before_count = vector_store.client.ntotal\n",
        "\n",
        "            # Update index with new documents\n",
        "            for doc in documents:\n",
        "                index.insert(doc)  # Update index with new documents\n",
        "\n",
        "            # Count vectors AFTER\n",
        "            after_count = vector_store.client.ntotal\n",
        "\n",
        "            #logging.info(\"Inserted new documents into existing FAISS index.\")\n",
        "            logging.info(f\"Inserted {newly_added} new docs. Vectors before: {before_count}, after: {after_count}\")\n",
        "        else:\n",
        "            # Create a new index\n",
        "            vector_store = FaissVectorStore(faiss.IndexFlatIP(embedding_dimension))\n",
        "            storage_context = StorageContext.from_defaults(\n",
        "                vector_store=vector_store,\n",
        "                docstore=SimpleDocumentStore(),\n",
        "                index_store=SimpleIndexStore(),\n",
        "                persist_dir=str(index_path),\n",
        "            )\n",
        "            index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, embedding=embedding_model) # Create index from scratch\n",
        "\n",
        "            # Count vectors BEFORE\n",
        "            before_count = 0\n",
        "\n",
        "            # Count vectors AFTER\n",
        "            after_count = vector_store.client.ntotal\n",
        "\n",
        "            #logging.info(\"Inserted new documents into a freshly created FAISS index.\")\n",
        "            logging.info(\n",
        "                        f\"Created a new FAISS index and inserted {newly_added} docs. \"\n",
        "                        f\"Vectors before: 0, after: {after_count}\"\n",
        "                    )\n",
        "        # Persist the created/updated index\n",
        "        index.storage_context.persist(\n",
        "            persist_dir=str(index_path),\n",
        "            vector_store_fname=\"vector_store.faiss\",  # Important to save with .faiss suffix\n",
        "        )\n",
        "        logging.info(\"FAISS index created/updated and persisted successfully.\")\n",
        "\n",
        "    else:\n",
        "        logging.info(\"No documents to index for FAISS.\")\n",
        "\n",
        "# Execute FAISS indexing\n",
        "create_or_update_faiss_index(all_new_docs, faiss_index_path, embedding_model, embedding_dimension)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXjNk6c9YRBh",
        "outputId": "0ece6e82-08c2-459e-ca52-212f223211db"
      },
      "id": "nXjNk6c9YRBh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-02 11:34:36,380 - INFO - HTTP Request: POST https://api.studio.nebius.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-01-02 11:34:36,505 - WARNING - No existing llama_index.core.graph_stores.simple found at /content/gdrive/MyDrive/RAG_Project_5/data/faiss_index/graph_store.json. Initializing a new graph_store from scratch. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Embedding Model BAAI/bge-en-icl creates 4096-dimensional vectors.\n",
            "\n",
            "Sample embedding for text 'test text to create an embedding':\n",
            "[0.004482269287109375, -0.00551605224609375, 0.008056640625, 0.0103302001953125, -0.025604248046875, -0.004878997802734375, -0.01409912109375, 0.0168914794921875, 0.0027904510498046875, -0.00604248046875] ...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-02 11:34:36,727 - INFO - HTTP Request: POST https://api.studio.nebius.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-01-02 11:34:37,283 - INFO - HTTP Request: POST https://api.studio.nebius.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-01-02 11:34:37,599 - INFO - HTTP Request: POST https://api.studio.nebius.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-01-02 11:34:37,812 - INFO - HTTP Request: POST https://api.studio.nebius.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-01-02 11:34:38,016 - INFO - HTTP Request: POST https://api.studio.nebius.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-01-02 11:34:38,064 - INFO - Created a new FAISS index and inserted 49 docs. Vectors before: 0, after: 49\n",
            "2025-01-02 11:34:38,113 - INFO - FAISS index created/updated and persisted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Move processed files to a designated folder and update the records of processed files\n",
        "\n",
        "After successful ingestion and indexing, moves newly processed files from “to_process” into the “processed” folder. Also updates the JSON records of processed files to include these newly completed items, preventing redundant reprocessing in the future."
      ],
      "metadata": {
        "id": "jEXZO-_x0huy"
      },
      "id": "jEXZO-_x0huy"
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Move processed files to a designated folder and update the records of processed files to keep track of which files have already been processed\n",
        "\n",
        "def move_files_to_processed(files: List[Path], destination_dir: Path) -> None:\n",
        "    \"\"\"\n",
        "    Moves processed files to the designated 'processed' directory.\n",
        "\n",
        "    Args:\n",
        "        files (List[Path]): List of file paths to move.\n",
        "        destination_dir (Path): Destination directory where files will be moved.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    for file in files:\n",
        "        try:\n",
        "            target_path = destination_dir / file.relative_to(to_process_dir)  # Maintain folder structure\n",
        "            target_path.parent.mkdir(parents=True, exist_ok=True)  # Create parent folders if needed\n",
        "            file.rename(target_path)\n",
        "            logging.info(f\"Moved {file.name} to {target_path}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to move file {file.name} to {target_path}: {e}\")\n",
        "\n",
        "def update_processed_files(record_path: Path, file_list: List[Path]) -> None:\n",
        "    \"\"\"\n",
        "    Updates the record of processed files (JSON files that keep track of files being processed).\n",
        "\n",
        "    Adds the names of the files that have just been processed to a JSON record.\n",
        "\n",
        "    Args:\n",
        "        record_path (Path): Path to the JSON record file.\n",
        "        file_list (List[Path]): List of file paths that have been processed.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    try:\n",
        "        processed = set()\n",
        "        if record_path.exists():\n",
        "            with open(record_path, 'r') as f:\n",
        "                processed = set(json.load(f))\n",
        "        processed.update([file.name for file in file_list])\n",
        "        with open(record_path, 'w') as f:\n",
        "            json.dump(list(processed), f)\n",
        "        logging.info(f\"Updated processed files record at {record_path}.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error updating processed files record at {record_path}: {e}\")\n",
        "\n",
        "# Move each type of processed files to /processed/\n",
        "move_files_to_processed(new_files_pdf, processed_dir)\n",
        "move_files_to_processed(new_files_excel, processed_dir)\n",
        "move_files_to_processed(new_files_ppt, processed_dir)\n",
        "move_files_to_processed(new_files_doc, processed_dir)\n",
        "\n",
        "# Update the processed files record for each file type\n",
        "update_processed_files(processed_files_record_pdf, new_files_pdf)\n",
        "update_processed_files(processed_files_record_excel, new_files_excel)\n",
        "update_processed_files(processed_files_record_ppt, new_files_ppt)\n",
        "update_processed_files(processed_files_record_doc, new_files_doc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cGpaCLjErLU",
        "outputId": "1ed3b488-6898-41ee-c056-e19b48c49a6a"
      },
      "id": "7cGpaCLjErLU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-02 11:34:43,557 - INFO - Moved Lessons Learned_2021 BOHAI_ZPEC Campaign_MD_short.xlsx to /content/gdrive/MyDrive/RAG_Project_5/data/processed/Lessons Learned_2021 BOHAI_ZPEC Campaign_MD_short.xlsx\n",
            "2025-01-02 11:34:43,568 - INFO - Updated processed files record at /content/gdrive/MyDrive/RAG_Project_5/data/processed/processed_files_pdf.json.\n",
            "2025-01-02 11:34:43,578 - INFO - Updated processed files record at /content/gdrive/MyDrive/RAG_Project_5/data/processed/processed_files_excel.json.\n",
            "2025-01-02 11:34:43,588 - INFO - Updated processed files record at /content/gdrive/MyDrive/RAG_Project_5/data/processed/processed_files_ppt.json.\n",
            "2025-01-02 11:34:43,599 - INFO - Updated processed files record at /content/gdrive/MyDrive/RAG_Project_5/data/processed/processed_files_doc.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. Display entries from the processed documents for verification\n",
        "\n",
        "Shows a sample of newly ingested documents (like PDF pages, Excel rows, slides, Word chunks) with their metadata. Useful for quick verification that text chunks and metadata have been correctly split and associated. Only a few entries (but one can manage exact number and location of chunks) are displayed to ensure the pipeline's correctness without overwhelming output."
      ],
      "metadata": {
        "id": "sk2pQV0M05ID"
      },
      "id": "sk2pQV0M05ID"
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Display text chunks and metadata created\n",
        "\n",
        "import json\n",
        "import textwrap\n",
        "from typing import List\n",
        "from llama_index.core import Document\n",
        "\n",
        "total_new_docs = len(all_new_docs)\n",
        "print(f\"Total new documents created: {total_new_docs}\\n\")\n",
        "\n",
        "def display_entries(entries: List[Document], file_type: str, start: int = 0, num: int = 5, text_width: int = 100, truncate: bool = False, max_length: int = 1500):\n",
        "    \"\"\"\n",
        "    Displays specified entries of metadata for verification.\n",
        "    The 'text' field (LlamaIndex Document) is displayed in a wrapped and readable format.\n",
        "\n",
        "    Args:\n",
        "        entries (List[Document]): List of Document objects.\n",
        "        file_type (str): The type of documents (e.g., 'PDF', 'Excel', 'PPT', 'DOC').\n",
        "        start (int, optional): Starting index. Defaults to 0.\n",
        "        num (int, optional): Number of entries to display. Defaults to 5.\n",
        "        text_width (int, optional): Width for text wrapping. Defaults to 100.\n",
        "        truncate (bool, optional): Whether to truncate long texts. Defaults to False.\n",
        "        max_length (int, optional): Maximum length of text to display if truncating. Defaults to 1500.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    print(f\"Displaying {num} Entries starting from index {start} in {file_type} Documents:\\n\")\n",
        "\n",
        "    end = start + num\n",
        "    for i, doc in enumerate(entries[start:end], start=start + 1):\n",
        "        print(f\"Entry {i}:\")\n",
        "        metadata = doc.metadata\n",
        "        print(json.dumps(metadata, indent=4))\n",
        "\n",
        "        text = doc.text\n",
        "        if text:\n",
        "            print(\"\\nText:\")\n",
        "            if truncate and len(text) > max_length:\n",
        "                displayed_text = text[:max_length] + \"...\\n[Text truncated]\"\n",
        "            else:\n",
        "                displayed_text = text\n",
        "            wrapped_text = textwrap.fill(displayed_text, width=text_width)\n",
        "            print(wrapped_text)\n",
        "\n",
        "        print(\"\\n\" + \"-\" * text_width + \"\\n\")\n",
        "\n",
        "# Display entries for each document type\n",
        "display_entries(pdf_docs, 'PDF')\n",
        "display_entries(excel_docs, 'Excel', start=0, num=5)\n",
        "display_entries(ppt_docs, 'PPT', start=0, num=5)\n",
        "display_entries(doc_docs, 'DOC', start=0, num=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gxixAEuVE7dN",
        "outputId": "0268d62e-b0be-4fe5-8c77-cdae2928bda9"
      },
      "id": "gxixAEuVE7dN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total new documents created: 49\n",
            "\n",
            "Displaying 5 Entries starting from index 0 in PDF Documents:\n",
            "\n",
            "Displaying 5 Entries starting from index 0 in Excel Documents:\n",
            "\n",
            "Entry 1:\n",
            "{\n",
            "    \"sheet\": \"Lessons Learned and Proposals\",\n",
            "    \"row_number\": 1,\n",
            "    \"doc_id\": \"84f2b87d-8701-4227-a6dc-907b13727d8c\",\n",
            "    \"source\": \"Lessons Learned_2021 BOHAI_ZPEC Campaign_MD_short.xlsx\"\n",
            "}\n",
            "\n",
            "Text:\n",
            "##: 1.1, type: General, reference: All, subject: KPI Matrix for Contractor's performance,\n",
            "description: Assessment of the Contractor's performance as per newly developed KPI matrix.\n",
            "Acceptance of problematic wells with discount as per matrix, which include: Cement quality, Survey &\n",
            "Logging quality, Trajectory and target accuracy, Programs preparation & reports., lessons learned:\n",
            "nan, action: Contract's App update required\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Entry 2:\n",
            "{\n",
            "    \"sheet\": \"Lessons Learned and Proposals\",\n",
            "    \"row_number\": 2,\n",
            "    \"doc_id\": \"58b1abf9-f4bd-4d90-8dc0-3f87d723c2a0\",\n",
            "    \"source\": \"Lessons Learned_2021 BOHAI_ZPEC Campaign_MD_short.xlsx\"\n",
            "}\n",
            "\n",
            "Text:\n",
            "##: 1.2, type: General, reference: All, subject: Engineering Software, description: Contractor\n",
            "should have appropriate Engineering software for: Cementing simulation, T&D, Hydraulics, Well\n",
            "Planning etc. as per our requirements, lessons learned: nan, action: Contract's App update required\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Entry 3:\n",
            "{\n",
            "    \"sheet\": \"Lessons Learned and Proposals\",\n",
            "    \"row_number\": 3,\n",
            "    \"doc_id\": \"3232dcab-2b13-42b9-92b1-0d37029ce217\",\n",
            "    \"source\": \"Lessons Learned_2021 BOHAI_ZPEC Campaign_MD_short.xlsx\"\n",
            "}\n",
            "\n",
            "Text:\n",
            "##: 1.3, type: General, reference: WQ2-831 (coring), WQ2-678 (LNR HGR service), and others, subject:\n",
            "In-house equipment, description: Based on the recent experience, we should pay attention and be\n",
            "extremely cautious to any kind of “in-house” services and/or equipment being provided by Drilling\n",
            "Contractor such as Coring, Liner Hangers, Directional Drilling or any other critical stuff., lessons\n",
            "learned: nan, action: Probably contract's App update required Consider whilst preparation for\n",
            "drilling\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Entry 4:\n",
            "{\n",
            "    \"sheet\": \"Lessons Learned and Proposals\",\n",
            "    \"row_number\": 4,\n",
            "    \"doc_id\": \"d1f9fdea-0d5a-42f1-a9cf-d33a89a094e3\",\n",
            "    \"source\": \"Lessons Learned_2021 BOHAI_ZPEC Campaign_MD_short.xlsx\"\n",
            "}\n",
            "\n",
            "Text:\n",
            "##: 1.4, type: General, reference: WQ2-831 (coring), WQ2-678 (LNR HGR service), and others, subject:\n",
            "Back-ups and alternatives, description: For any kind of critical operations, there should always be\n",
            "a choice between different service suppliers (at least two providers) with well-known service\n",
            "quality and reliable equipment (as proven by case histories)., lessons learned: nan, action:\n",
            "Contract's App update required. Consider whilst preparation for drilling\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Entry 5:\n",
            "{\n",
            "    \"sheet\": \"Lessons Learned and Proposals\",\n",
            "    \"row_number\": 5,\n",
            "    \"doc_id\": \"3c06e4ae-28c9-444f-8dcf-38b0e26e647d\",\n",
            "    \"source\": \"Lessons Learned_2021 BOHAI_ZPEC Campaign_MD_short.xlsx\"\n",
            "}\n",
            "\n",
            "Text:\n",
            "##: 1.5, type: General, reference: WQ2-787, subject: H2S Safety procedures, personnel and equipment,\n",
            "description: Experienced serious problems while preparing documents such as Emergency Response Plan,\n",
            "Site Specific Safety Plan, etc. H2S Safety Equipment (such as Air Loop system, Breathing Apparatus)\n",
            "didn’t meet LME HSE dept requirements after being phisically checked on site, despite specs of the\n",
            "equipment were provided to HSE in advance. Subcontractor H2S Engineer wasn't properly certified as\n",
            "required by LME HSE requirements. Finally Drilling Contractor had to change H2S Services company to\n",
            "another one., lessons learned: To avoid any delay due to unavailability of proper equipment or\n",
            "personnel in the Country, LME HSE department should share  responsibility  and be actively involved\n",
            "in preparation at the early stage., action: Clearly specify H2S Safety requirement in the Contract\n",
            "(involve HSE dept.)\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Displaying 5 Entries starting from index 0 in PPT Documents:\n",
            "\n",
            "Displaying 5 Entries starting from index 0 in DOC Documents:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "APPENDIXES:\n",
        "\n",
        "Print all the codecells from the pipeline above - to use this code (all the code cells together) later to create a .py file for production\n",
        "\n",
        "Skip only cells marked \"# SKIP ME\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BQDC8U-XZKCD"
      },
      "id": "BQDC8U-XZKCD"
    },
    {
      "cell_type": "code",
      "source": [
        "# SKIP ME\n",
        "\n",
        "# APPENDIXES: Print all the codecells from the pipeline above (skips cells marked \"# SKIP ME\")\n",
        "\n",
        "import json\n",
        "\n",
        "# Fetch the current notebook's metadata and contents\n",
        "from google.colab import _message\n",
        "\n",
        "def get_notebook_content():\n",
        "    try:\n",
        "        notebook = _message.blocking_request('get_ipynb', timeout_sec=5)\n",
        "        return notebook['ipynb']\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching notebook content: {e}\")\n",
        "        return None\n",
        "\n",
        "# Get the notebook content\n",
        "notebook_data = get_notebook_content()\n",
        "\n",
        "if notebook_data:\n",
        "    code_cells = [\n",
        "        \"\".join(cell[\"source\"])\n",
        "        for cell in notebook_data.get(\"cells\", [])\n",
        "        if cell[\"cell_type\"] == \"code\" and \"# SKIP ME\" not in \"\".join(cell[\"source\"])\n",
        "    ]\n",
        "\n",
        "    # Join the code cells with three empty lines between them\n",
        "    readable_code = \"\\n\\n\\n\".join(code_cells)\n",
        "\n",
        "#    print(\"Extracted Code Cells:\\n\\n\\n\")\n",
        "    print(readable_code)\n",
        "else:\n",
        "    print(\"Could not fetch notebook content.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "v97iI2q81hCe",
        "outputId": "6bd17003-9117-4290-e233-4979d380eda9"
      },
      "id": "v97iI2q81hCe",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#1. Install required libraries\n",
            "!pip install pdfplumber pandas openpyxl bleach faiss-cpu openai\n",
            "!pip install -U llama-index llama-index-core llama-parse llama-index-readers-file openai\n",
            "!pip install llama-index-embeddings-nebius\n",
            "!pip install llama-index-vector-stores-faiss\n",
            "!pip install python-pptx python-docx\n",
            "!pip install whoosh\n",
            "\n",
            "\n",
            "#2. Configuration: Mount Google Drive and config environment\n",
            "\n",
            "from google.colab import drive\n",
            "drive.mount('/content/gdrive')\n",
            "\n",
            "import os\n",
            "from pathlib import Path\n",
            "import logging\n",
            "import json\n",
            "import openai  # Import openai to set api_base globally\n",
            "\n",
            "# Logging configuration parameters\n",
            "logging_level = logging.INFO\n",
            "logging_format = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
            "log_folder=Path(\"/content/gdrive/MyDrive/RAG_Project_5/data_preparation/\")\n",
            "log_file=\"data_preparation.log\"\n",
            "max_bytes=5 * 1024 * 1024  # 5 MB\n",
            "backup_count=3  # Keep up to 3 backup files\n",
            "\n",
            "\n",
            "# Define directories for file processing and validate the directories exist\n",
            "data_directory = Path('/content/gdrive/MyDrive/RAG_Project_5/data')\n",
            "to_process_dir = data_directory / 'to_process'\n",
            "processed_dir = data_directory / 'processed'\n",
            "\n",
            "for folder in [to_process_dir, processed_dir]:\n",
            "    folder.mkdir(parents=True, exist_ok=True)  # Create if not exists\n",
            "\n",
            "# Define directories for indexes\n",
            "whoosh_index_path = data_directory / \"whoosh_index\"\n",
            "os.makedirs(whoosh_index_path, exist_ok=True)\n",
            "\n",
            "faiss_index_path = data_directory / \"faiss_index\"\n",
            "os.makedirs(faiss_index_path, exist_ok=True)\n",
            "\n",
            "\n",
            "\n",
            "# Path to the JSON config file\n",
            "config_path = '/content/gdrive/MyDrive/Colab_Notebooks/config.json'\n",
            "\n",
            "# Load the API key (Nebius AI)\n",
            "with open(config_path, encoding='utf-8-sig') as config_file:\n",
            "    config = json.load(config_file)\n",
            "    os.environ['API_KEY'] = config['API_KEY']\n",
            "\n",
            "# Set the API key and API base globally\n",
            "openai.api_key = os.environ.get(\"API_KEY\")\n",
            "openai.api_base = \"https://api.studio.nebius.ai/v1/\"  # Nebius AI endpoint\n",
            "\n",
            "# Chunk parameters for word documents\n",
            "max_tokens=1024\n",
            "overlap_tokens=50\n",
            "\n",
            "# Define embedding model from Nebius AI Studio\n",
            "model_name = \"BAAI/bge-en-icl\" # from Nebius AI Studio\n",
            "\n",
            "\n",
            "#3. Logging Configuration\n",
            "\n",
            "import logging\n",
            "from logging.handlers import RotatingFileHandler\n",
            "from pathlib import Path\n",
            "import os\n",
            "\n",
            "def setup_logging(\n",
            "    log_folder: Path,\n",
            "    log_file: str = \"data_preparation.log\",\n",
            "    max_bytes: int = 5 * 1024 * 1024,  # 5 MB\n",
            "    backup_count: int = 3,\n",
            ") -> None:\n",
            "    \"\"\"\n",
            "    Configures logging with log rotation to write logs to both a file and the console.\n",
            "\n",
            "    Args:\n",
            "        log_folder (Path): The directory where the log file will be stored.\n",
            "        log_file (str, optional): The name of the log file. Defaults to \"data_preparation.log\".\n",
            "        max_bytes (int, optional): Maximum size of the log file in bytes before it is rotated. Defaults to 5 MB.\n",
            "        backup_count (int, optional): Number of backup files to keep (when log file exides max_bytes size, it becomes a backup file). Defaults to 3.\n",
            "\n",
            "    Returns:\n",
            "        None\n",
            "    \"\"\"\n",
            "    os.makedirs(log_folder, exist_ok=True)  # Ensure the folder exists\n",
            "    log_file_path = log_folder / log_file\n",
            "\n",
            "    # Create a RotatingFileHandler\n",
            "    rotating_handler = RotatingFileHandler(\n",
            "        filename=log_file_path,\n",
            "        maxBytes=max_bytes,\n",
            "        backupCount=backup_count,\n",
            "        encoding=\"utf-8\",\n",
            "    )\n",
            "\n",
            "    # Configure logging\n",
            "    logging.basicConfig(\n",
            "        level = logging_level,\n",
            "        format = logging_format,\n",
            "        handlers=[\n",
            "            rotating_handler,  # Handles log rotation\n",
            "            logging.StreamHandler(),  # Logs also appear in Colab's output\n",
            "        ],\n",
            "        force=True,  # Force the configuration to apply even if logging was already configured\n",
            "    )\n",
            "\n",
            "    logging.info(\"Logging with rotation has been successfully configured.\")\n",
            "\n",
            "# Initialize logging with log rotation\n",
            "setup_logging(\n",
            "    log_folder=log_folder,\n",
            "    log_file=log_file,\n",
            "    max_bytes=max_bytes,\n",
            "    backup_count=backup_count,\n",
            ")\n",
            "\n",
            "\n",
            "# 4. Load already processed files and validate new files for processing\n",
            "\n",
            "import json\n",
            "from pathlib import Path\n",
            "from typing import Set, List\n",
            "\n",
            "def load_processed_files(record_path: Path) -> Set[str]:\n",
            "    \"\"\"\n",
            "    Loads the set of processed files from a JSON record.\n",
            "\n",
            "    Args:\n",
            "        record_path (Path): Path to the JSON file containing processed file names.\n",
            "\n",
            "    Returns:\n",
            "        Set[str]: A set of processed file names.\n",
            "        (In Python, a set is an unordered collection of unique item, no duplicates are allowed;\n",
            "        operations like adding, removing, and checking membership are efficient with sets)\n",
            "    \"\"\"\n",
            "    if record_path.exists():\n",
            "        with open(record_path, 'r') as f:\n",
            "            processed = set(json.load(f)) # converts the list into a set (checks and removes duplicates)\n",
            "        logging.info(f\"Loaded processed files from {record_path}.\")\n",
            "    else:\n",
            "        processed = set()\n",
            "        logging.info(f\"No processed files record found at {record_path}. Starting fresh.\")\n",
            "    return processed\n",
            "\n",
            "def log_new_files(file_list: List[Path], file_type: str) -> None:\n",
            "    \"\"\"\n",
            "    Logs and prints a summary of new files to process.\n",
            "\n",
            "    Args:\n",
            "        file_list (List[Path]): List of new files to process.\n",
            "        file_type (str): The type of files (e.g., 'PDF', 'Excel', 'PPT', 'DOC').\n",
            "\n",
            "    Returns:\n",
            "        None\n",
            "    \"\"\"\n",
            "    if not file_list:\n",
            "        logging.info(f\"No new {file_type.upper()} files found in {to_process_dir}.\")\n",
            "        print(f\"No new {file_type.upper()} files to process.\\n\")\n",
            "    else:\n",
            "        logging.info(f\"Found {len(file_list)} new {file_type.upper()} files to process.\")\n",
            "        print(f\"New {file_type.upper()} Files Found:\")\n",
            "        for file in file_list:\n",
            "            print(file)\n",
            "        print()\n",
            "\n",
            "# Define paths for records of processed files per file type\n",
            "processed_files_record_pdf = processed_dir / \"processed_files_pdf.json\"\n",
            "processed_files_record_excel = processed_dir / \"processed_files_excel.json\"\n",
            "processed_files_record_ppt = processed_dir / \"processed_files_ppt.json\"\n",
            "processed_files_record_doc = processed_dir / \"processed_files_doc.json\"\n",
            "\n",
            "# Load processed files\n",
            "processed_files_pdf = load_processed_files(processed_files_record_pdf)\n",
            "processed_files_excel = load_processed_files(processed_files_record_excel)\n",
            "processed_files_ppt = load_processed_files(processed_files_record_ppt)\n",
            "processed_files_doc = load_processed_files(processed_files_record_doc)\n",
            "\n",
            "\n",
            "# Supported extensions for processing\n",
            "supported_extensions = [\".pdf\", \".xls\", \".xlsx\", \".ppt\", \".pptx\", \".doc\", \".docx\"]\n",
            "\n",
            "# List all relevant files recursively from nested folders\n",
            "all_files = [f for f in to_process_dir.rglob(\"*\") if f.is_file() and f.suffix.lower() in supported_extensions]\n",
            "\n",
            "# Categorize files by type\n",
            "files_pdf = [f for f in all_files if f.suffix.lower() == \".pdf\"]\n",
            "files_excel = [f for f in all_files if f.suffix.lower() in [\".xls\", \".xlsx\"]]\n",
            "#files_ppt = [f for f in all_files if f.suffix.lower() == \".ppt\"]\n",
            "files_ppt = [f for f in all_files if f.suffix.lower() in [\".ppt\", \".pptx\"]]\n",
            "files_doc = [f for f in all_files if f.suffix.lower() in [\".doc\", \".docx\"]]\n",
            "\n",
            "# Identify new files for processing\n",
            "new_files_pdf = [f for f in files_pdf if f.name not in processed_files_pdf]\n",
            "new_files_excel = [f for f in files_excel if f.name not in processed_files_excel]\n",
            "new_files_ppt = [f for f in files_ppt if f.name not in processed_files_ppt]\n",
            "new_files_doc = [f for f in files_doc if f.name not in processed_files_doc]\n",
            "\n",
            "# Validate new files\n",
            "log_new_files(new_files_pdf, \"PDF\")\n",
            "log_new_files(new_files_excel, \"Excel\")\n",
            "log_new_files(new_files_ppt, \"PPT\")\n",
            "log_new_files(new_files_doc, \"DOC\")\n",
            "\n",
            "\n",
            "# 5 Setup of utility functions for Data Ingestion & Preprocessing Sections\n",
            "\n",
            "from llama_index.core import Document\n",
            "from pathlib import Path\n",
            "from typing import List\n",
            "import uuid\n",
            "\n",
            "# Functions are used in preprocessing PDF and WORD files\n",
            "def assign_doc_id(documents: list) -> list:\n",
            "    \"\"\"\n",
            "    Assigns a unique doc_id to each document, adds this doc_id to metadata.\n",
            "\n",
            "    Args:\n",
            "        documents (list): List of Document objects.\n",
            "\n",
            "    Returns:\n",
            "        list: List of Document objects with assigned doc_id.\n",
            "    \"\"\"\n",
            "    for doc in documents:\n",
            "        # Assign a unique ID\n",
            "        doc_id = str(uuid.uuid4())\n",
            "        doc.metadata[\"doc_id\"] = doc_id\n",
            "    return documents\n",
            "\n",
            "def extract_metadata(doc: Document, source_file: str, additional_info: dict) -> Document:\n",
            "    \"\"\"\n",
            "    Updates Document metadata with additional information: source file name and other key-value pairs from additional info.\n",
            "\n",
            "    Args:\n",
            "        doc (Document): The document object.\n",
            "        source_file (str): The source file name.\n",
            "        additional_info (dict): Additional metadata information.\n",
            "\n",
            "    Returns:\n",
            "        Document: Document object with updated metadata.\n",
            "    \"\"\"\n",
            "    doc.metadata[\"source\"] = source_file\n",
            "    for key, value in additional_info.items():\n",
            "        doc.metadata[key] = value\n",
            "    return doc\n",
            "\n",
            "\n",
            "# 6. Data Ingestion & Preprocessing for PDF\n",
            "\n",
            "from llama_index.core import Document\n",
            "from llama_index.readers.file import PDFReader\n",
            "from pathlib import Path\n",
            "from typing import List\n",
            "# import uuid\n",
            "\n",
            "def load_pdf_docs(pdf_paths: List[Path]) -> List[Document]:\n",
            "    \"\"\"\n",
            "    Uses LlamaIndex PDFReader to load text content of PDF files\n",
            "    and organizes them into Document objects.\n",
            "\n",
            "    Each page of a PDF file (with associated metadata) will be a separate Document.\n",
            "\n",
            "    Args:\n",
            "        pdf_paths (List[Path]): List of PDF file paths to process.\n",
            "\n",
            "    Returns:\n",
            "        List[Document]: A list of Document objects containing the text and metadata from the PDFs.\n",
            "    \"\"\"\n",
            "    pdf_reader = PDFReader()\n",
            "    all_docs = []\n",
            "    for pdf_path in pdf_paths:\n",
            "        try:\n",
            "            docs = pdf_reader.load_data(pdf_path)\n",
            "            docs = assign_doc_id(docs)  # Assign doc_id and add doc_id to metadata\n",
            "            # Extract additional metadata if available\n",
            "            for doc in docs:\n",
            "                additional_info = {\n",
            "                  #  \"page_number\": doc.metadata.get(\"page_number\"), # no need for page number here - \"page label\" will be extracted automatically\n",
            "                    # Add other metadata fields as necessary\n",
            "                }\n",
            "                doc = extract_metadata(doc, source_file=pdf_path.name, additional_info=additional_info)\n",
            "            all_docs.extend(docs)\n",
            "            logging.info(f\"Processed PDF: {pdf_path.name} with {len(docs)} pages.\")\n",
            "        except Exception as e:\n",
            "            logging.error(f\"Failed to process PDF {pdf_path.name}: {e}\")\n",
            "    return all_docs\n",
            "\n",
            "# Load pdf docs\n",
            "if new_files_pdf:\n",
            "    pdf_docs = load_pdf_docs(new_files_pdf)\n",
            "    logging.info(f\"Total new PDF documents loaded: {len(pdf_docs)}\")\n",
            "else:\n",
            "    pdf_docs = []\n",
            "    logging.info(\"No new PDF files to process.\")\n",
            "\n",
            "\n",
            "# 7. Data Ingestion & Preprocessing for Excel\n",
            "\n",
            "from llama_index.core import Document\n",
            "import pandas as pd\n",
            "from pathlib import Path\n",
            "from typing import List\n",
            "\n",
            "def load_excel_docs(excel_paths: List[Path]) -> List[Document]:\n",
            "    \"\"\"\n",
            "    Uses Pandas to load documents from Excel files and organizes them into Document objects.\n",
            "\n",
            "    Each row of an Excel sheet is converted into a Document with metadata.\n",
            "\n",
            "    Args:\n",
            "        excel_paths (List[Path]): List of Excel file paths to process.\n",
            "\n",
            "    Returns:\n",
            "        List[Document]: A list of Document objects containing the text and metadata from the Excel files.\n",
            "    \"\"\"\n",
            "    all_docs = []\n",
            "    for excel_path in excel_paths:\n",
            "        try:\n",
            "            xls = pd.ExcelFile(excel_path)\n",
            "\n",
            "            for sheet_name in xls.sheet_names:\n",
            "                df = pd.read_excel(xls, sheet_name=sheet_name)\n",
            "\n",
            "                for idx, row in df.iterrows():\n",
            "                    row_dict = \", \".join(f\"{col.strip().lower()}: {str(row[col]).strip()}\" for col in df.columns)\n",
            "                    metadata = {\n",
            "                        \"sheet\": sheet_name,\n",
            "                        \"row_number\": idx + 1,  # 1-based indexing\n",
            "                    }\n",
            "\n",
            "                    doc = Document(text=row_dict, metadata=metadata)\n",
            "                    all_docs.append(doc)\n",
            "\n",
            "            logging.info(f\"Processed Excel: {excel_path.name} with {len(df)} rows.\")\n",
            "        except Exception as e:\n",
            "            logging.error(f\"Failed to process Excel {excel_path.name}: {e}\")\n",
            "\n",
            "    # Assign doc_ids and extract metadata\n",
            "    all_docs = assign_doc_id(all_docs)\n",
            "    for doc in all_docs:\n",
            "        additional_info = {}  # Add any additional metadata if necessary\n",
            "        doc = extract_metadata(doc, source_file=excel_path.name, additional_info=additional_info)\n",
            "    return all_docs\n",
            "\n",
            "if new_files_excel:\n",
            "    excel_docs = load_excel_docs(new_files_excel)\n",
            "    logging.info(f\"Total new Excel documents loaded: {len(excel_docs)}\")\n",
            "else:\n",
            "    excel_docs = []\n",
            "    logging.info(\"No new Excel files to process.\")\n",
            "\n",
            "\n",
            "# 8. Data Ingestion & Preprocessing for PowerPoint\n",
            "\n",
            "from pptx import Presentation\n",
            "from pathlib import Path\n",
            "from typing import List\n",
            "from llama_index.core import Document\n",
            "\n",
            "def load_ppt_docs(ppt_paths: List[Path]) -> List[Document]:\n",
            "    \"\"\"\n",
            "    Loads text content from PowerPoint files and organizes them into Document objects.\n",
            "\n",
            "    Each slide is converted into a Document with metadata.\n",
            "\n",
            "    Args:\n",
            "        ppt_paths (List[Path]): List of PowerPoint file paths to process.\n",
            "\n",
            "    Returns:\n",
            "        List[Document]: A list of Document objects containing the text and metadata from the slides.\n",
            "    \"\"\"\n",
            "    all_docs = []\n",
            "    for ppt_path in ppt_paths:\n",
            "        try:\n",
            "            prs = Presentation(ppt_path)\n",
            "            for idx, slide in enumerate(prs.slides):\n",
            "                slide_text = \"\\n\".join(shape.text for shape in slide.shapes if hasattr(shape, \"text\"))  # Collects only text from the slide\n",
            "                metadata = {\n",
            "                    \"slide_number\": idx + 1,\n",
            "                }\n",
            "                doc = Document(text=slide_text, metadata=metadata)\n",
            "                all_docs.append(doc)\n",
            "            logging.info(f\"Processed PowerPoint: {ppt_path.name} with {len(prs.slides)} slides.\")\n",
            "        except Exception as e:\n",
            "            logging.error(f\"Failed to process PowerPoint {ppt_path.name}: {e}\")\n",
            "\n",
            "    # Assign doc_ids and extract metadata\n",
            "    all_docs = assign_doc_id(all_docs)\n",
            "    for doc in all_docs:\n",
            "        additional_info = {}  # Add any additional metadata if necessary\n",
            "        doc = extract_metadata(doc, source_file=ppt_path.name, additional_info=additional_info)\n",
            "    return all_docs\n",
            "\n",
            "if new_files_ppt:\n",
            "    ppt_docs = load_ppt_docs(new_files_ppt)\n",
            "    logging.info(f\"Total new PowerPoint documents loaded: {len(ppt_docs)}\")\n",
            "else:\n",
            "    ppt_docs = []\n",
            "    logging.info(\"No new PowerPoint files to process.\")\n",
            "\n",
            "\n",
            "# 9. Data Ingestion & Preprocessing for Word\n",
            "\n",
            "# !!! We need to use functions: assing_doc_id and extract_metadata here this word section is the only one which doesn't use these utility functions.\n",
            "\n",
            "from docx import Document as WordDocument\n",
            "from pathlib import Path\n",
            "from typing import List\n",
            "from llama_index.core import Document\n",
            "from transformers import GPT2TokenizerFast\n",
            "import uuid\n",
            "import textwrap\n",
            "\n",
            "# Initialize tokenizer for token-based chunking\n",
            "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
            "\n",
            "def split_large_section(\n",
            "    section: List[str], metadata: dict, max_tokens: int, overlap_tokens: int\n",
            ") -> List[Document]:\n",
            "    \"\"\"\n",
            "    Splits a large text section into smaller chunks based on token count.\n",
            "\n",
            "    Args:\n",
            "        section (List[str]): List of paragraphs in the section.\n",
            "        metadata (dict): Metadata associated with the section.\n",
            "        max_tokens (int): Maximum number of tokens per chunk.\n",
            "        overlap_tokens (int): Number of overlapping tokens between chunks.\n",
            "\n",
            "    Returns:\n",
            "        List[Document]: List of smaller Document objects.\n",
            "    \"\"\"\n",
            "    all_docs = []\n",
            "    text = \"\\n\".join(section)\n",
            "    tokens = tokenizer.encode(text)\n",
            "    total_chunks = (len(tokens) + max_tokens - 1) // max_tokens  # Calculate total chunks\n",
            "\n",
            "    for chunk_number, i in enumerate(range(0, len(tokens), max_tokens - overlap_tokens), start=1):\n",
            "        chunk_tokens = tokens[i : i + max_tokens]\n",
            "        chunk_text = tokenizer.decode(chunk_tokens)\n",
            "\n",
            "        # Add metadata for the current chunk\n",
            "        chunk_metadata = {\n",
            "            **(metadata or {}),\n",
            "            \"chunk_number\": chunk_number,\n",
            "            \"total_chunks_in_section\": total_chunks,\n",
            "            \"start_token\": i,\n",
            "            \"end_token\": min(i + max_tokens, len(tokens)),\n",
            "            \"chunk_type\": \"token_chunk\",\n",
            "            \"doc_id\": str(uuid.uuid4()),  # Assign unique doc_id for each chunk\n",
            "        }\n",
            "\n",
            "        doc = Document(text=chunk_text, metadata=chunk_metadata)\n",
            "        all_docs.append(doc)\n",
            "\n",
            "    return all_docs\n",
            "\n",
            "def chunk_by_headings(\n",
            "    word_doc: WordDocument, max_tokens: int = 1024, overlap_tokens: int = 50\n",
            ") -> List[Document]:\n",
            "    \"\"\"\n",
            "    Chunks a Word document into sections based on headings and splits large sections by tokens.\n",
            "\n",
            "    Args:\n",
            "        word_doc (WordDocument): The Word document object.\n",
            "        max_tokens (int, optional): Maximum number of tokens per chunk. Defaults to 1024.\n",
            "        overlap_tokens (int, optional): Number of overlapping tokens between chunks. Defaults to 50.\n",
            "\n",
            "    Returns:\n",
            "        List[Document]: List of Document objects containing text and metadata.\n",
            "    \"\"\"\n",
            "    all_docs = []\n",
            "    current_section = []\n",
            "    current_metadata = None\n",
            "\n",
            "    for paragraph in word_doc.paragraphs:\n",
            "        if paragraph.style.name.startswith(\"Heading\"):  # Detect headings\n",
            "            # Save the current section as chunks\n",
            "            if current_section:\n",
            "                all_docs.extend(\n",
            "                    split_large_section(\n",
            "                        current_section, current_metadata, max_tokens, overlap_tokens\n",
            "                    )\n",
            "                )\n",
            "                current_section = []\n",
            "\n",
            "            # Update metadata for the new section\n",
            "            current_metadata = {\"section\": paragraph.text.strip()}\n",
            "\n",
            "        # Add paragraph text to the current section\n",
            "        current_section.append(paragraph.text.strip())\n",
            "\n",
            "    # Save the last section if it contains any content\n",
            "    if current_section:\n",
            "        all_docs.extend(\n",
            "            split_large_section(\n",
            "                current_section, current_metadata, max_tokens, overlap_tokens\n",
            "            )\n",
            "        )\n",
            "\n",
            "    return all_docs\n",
            "\n",
            "def load_doc_docs(\n",
            "    doc_paths: List[Path], max_tokens: int = 1024, overlap_tokens: int = 50\n",
            ") -> List[Document]:\n",
            "    \"\"\"\n",
            "    Processes Word documents by chunking them into sections based on headings.\n",
            "\n",
            "    Args:\n",
            "        doc_paths (List[Path]): List of Word document file paths to process.\n",
            "        max_tokens (int): Maximum number of tokens per chunk.\n",
            "        overlap_tokens (int): Number of overlapping tokens between chunks.\n",
            "\n",
            "    Returns:\n",
            "        List[Document]: List of Document objects containing the text and metadata from the Word files.\n",
            "    \"\"\"\n",
            "    all_docs = []\n",
            "    for doc_path in doc_paths:\n",
            "        try:\n",
            "            word_doc = WordDocument(doc_path)\n",
            "            logging.info(f\"Processing {doc_path.name} by headings.\")\n",
            "\n",
            "            # Use chunk_by_headings function\n",
            "            docs = chunk_by_headings(word_doc, max_tokens, overlap_tokens)\n",
            "\n",
            "            # Ensure all chunks include the source metadata\n",
            "            for doc in docs:\n",
            "                if 'source' not in doc.metadata:\n",
            "                    doc.metadata['source'] = doc_path.name  # Add source metadata if missing\n",
            "\n",
            "            all_docs.extend(docs)\n",
            "            logging.info(f\"Processed Word Document: {doc_path.name} with {len(docs)} chunks.\")\n",
            "        except Exception as e:\n",
            "            logging.error(f\"Failed to process Word Document {doc_path.name}: {e}\")\n",
            "    return all_docs\n",
            "\n",
            "if new_files_doc:\n",
            "    doc_docs = load_doc_docs(new_files_doc, max_tokens=max_tokens, overlap_tokens=overlap_tokens)\n",
            "    logging.info(f\"Total new Word documents loaded: {len(doc_docs)}\")\n",
            "else:\n",
            "    doc_docs = []\n",
            "    logging.info(\"No new Word files to process.\")\n",
            "\n",
            "\n",
            "\n",
            "# 10. Create / Update Whoosh Index - add new documents to it\n",
            "\n",
            "from whoosh.index import create_in, open_dir, exists_in\n",
            "from whoosh.fields import Schema, TEXT, ID, NUMERIC\n",
            "from whoosh import index\n",
            "from pathlib import Path\n",
            "import os\n",
            "import logging\n",
            "import uuid\n",
            "\n",
            "# Define the schema with all necessary metadata fields\n",
            "schema = Schema(\n",
            "    content=TEXT(stored=True),                  # Store the text content for search\n",
            "    source=ID(stored=True),                     # Store the document source as an identifier\n",
            "    sheet=ID(stored=True),                      # For Excel documents\n",
            "    row_number=NUMERIC(stored=True, sortable=True),\n",
            "    slide_number=NUMERIC(stored=True, sortable=True),  # For PPT documents\n",
            "    section=TEXT(stored=True),                  # For Word documents\n",
            "    chunk_number=NUMERIC(stored=True, sortable=True),\n",
            "    total_chunks_in_section=NUMERIC(stored=True, sortable=True),\n",
            "    page_number=NUMERIC(stored=True, sortable=True),    # For PDF documents\n",
            "    doc_id=ID(stored=True, unique=True),         # Unique identifier for each document\n",
            "    # Add other metadata fields as needed\n",
            ")\n",
            "\n",
            "def create_or_update_whoosh_index(documents: list, index_dir: Path) -> None:\n",
            "    \"\"\"\n",
            "    Creates or updates a Whoosh index with the given documents, including all metadata fields.\n",
            "\n",
            "    Args:\n",
            "        documents (list): List of Document objects to add to the index.\n",
            "        index_dir (Path): Directory where the Whoosh index will be stored.\n",
            "\n",
            "    Returns:\n",
            "        None\n",
            "    \"\"\"\n",
            "    try:\n",
            "        # Ensure the index directory exists\n",
            "        os.makedirs(index_dir, exist_ok=True)\n",
            "\n",
            "        # Check if an existing Whoosh index exists or create a new one\n",
            "        if not exists_in(index_dir):\n",
            "            # Create a new Whoosh index\n",
            "            idx = create_in(index_dir, schema)\n",
            "            logging.info(f\"Created a new Whoosh index at {index_dir}.\")\n",
            "            before_count = 0  # No documents in a newly created index\n",
            "        else:\n",
            "            # Open the existing Whoosh index\n",
            "            idx = open_dir(index_dir)\n",
            "            with idx.searcher() as searcher:\n",
            "                before_count = searcher.doc_count()  # Count existing documents\n",
            "            logging.info(f\"Opened existing Whoosh index at {index_dir}. Total docs before: {before_count}.\")\n",
            "\n",
            "        # Add or update documents in the index\n",
            "        writer = idx.writer()\n",
            "\n",
            "        for doc in documents:\n",
            "            try:\n",
            "                # Extract content and source from the document\n",
            "                content = doc.text\n",
            "                source = doc.metadata.get(\"source\", \"unknown_source\")\n",
            "                doc_id = doc.metadata.get(\"doc_id\", str(uuid.uuid4()))  # Ensure doc_id exists\n",
            "\n",
            "                # Prepare metadata fields\n",
            "                doc_fields = {\n",
            "                    \"content\": content,\n",
            "                    \"source\": source,\n",
            "                    \"doc_id\": doc_id,\n",
            "                }\n",
            "\n",
            "                # Conditionally add metadata fields if they exist\n",
            "                if \"sheet\" in doc.metadata:\n",
            "                    doc_fields[\"sheet\"] = doc.metadata[\"sheet\"]\n",
            "                if \"row_number\" in doc.metadata:\n",
            "                    doc_fields[\"row_number\"] = doc.metadata[\"row_number\"]\n",
            "                if \"slide_number\" in doc.metadata:\n",
            "                    doc_fields[\"slide_number\"] = doc.metadata[\"slide_number\"]\n",
            "                if \"section\" in doc.metadata:\n",
            "                    doc_fields[\"section\"] = doc.metadata[\"section\"]\n",
            "                if \"chunk_number\" in doc.metadata:\n",
            "                    doc_fields[\"chunk_number\"] = doc.metadata[\"chunk_number\"]\n",
            "                if \"total_chunks_in_section\" in doc.metadata:\n",
            "                    doc_fields[\"total_chunks_in_section\"] = doc.metadata[\"total_chunks_in_section\"]\n",
            "                if \"page_number\" in doc.metadata:\n",
            "                    doc_fields[\"page_number\"] = doc.metadata[\"page_number\"]\n",
            "\n",
            "                # Add or update document in the index\n",
            "                writer.update_document(**doc_fields)\n",
            "            except Exception as e:\n",
            "                logging.error(f\"Failed to index document {doc.metadata.get('doc_id', 'Unknown ID')}: {e}\")\n",
            "\n",
            "        # Commit changes to the index\n",
            "        writer.commit()\n",
            "\n",
            "        # Count documents after updating the index\n",
            "        with idx.searcher() as searcher:\n",
            "            after_count = searcher.doc_count()\n",
            "\n",
            "        # Log the document counts\n",
            "        newly_added = len(documents)\n",
            "        logging.info(\n",
            "            f\"Whoosh index updated. Docs before: {before_count}, docs added: {newly_added}, docs after: {after_count}.\"\n",
            "        )\n",
            "    except Exception as e:\n",
            "        logging.error(f\"Failed to create/update Whoosh index at {index_dir}: {e}\")\n",
            "        raise\n",
            "\n",
            "# We don't use backup_whoosh_index explicitly yet\n",
            "def backup_whoosh_index(whoosh_index_path: Path, backup_path: Path):\n",
            "    \"\"\"\n",
            "    Creates a backup of the Whoosh index.\n",
            "\n",
            "    Args:\n",
            "        whoosh_index_path (Path): Path to the Whoosh index.\n",
            "        backup_path (Path): Path to store the backup.\n",
            "\n",
            "    Returns:\n",
            "        None\n",
            "    \"\"\"\n",
            "    import shutil  # Imported here as per coding preference\n",
            "\n",
            "    try:\n",
            "        if whoosh_index_path.exists():\n",
            "            shutil.copytree(whoosh_index_path, backup_path, dirs_exist_ok=True)\n",
            "            logging.info(f\"Whoosh index backed up to {backup_path}.\")\n",
            "        else:\n",
            "            logging.warning(f\"Whoosh index path {whoosh_index_path} does not exist. Backup skipped.\")\n",
            "    except Exception as e:\n",
            "        logging.error(f\"Failed to backup Whoosh index: {e}\")\n",
            "\n",
            "# Define all_new_docs by aggregating all processed documents\n",
            "all_new_docs = pdf_docs + excel_docs + ppt_docs + doc_docs\n",
            "\n",
            "# Execute indexing\n",
            "# whoosh_index_path = data_directory / \"whoosh_index\" - moved to config section\n",
            "create_or_update_whoosh_index(all_new_docs, whoosh_index_path)\n",
            "\n",
            "\n",
            "# 11 Create / Update Faiss Index - add new documents to it\n",
            "\n",
            "import faiss\n",
            "import httpx\n",
            "from pathlib import Path\n",
            "from llama_index.embeddings.nebius import NebiusEmbedding\n",
            "from llama_index.vector_stores.faiss import FaissVectorStore\n",
            "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage\n",
            "from llama_index.core.storage.docstore.simple_docstore import SimpleDocumentStore\n",
            "from llama_index.core.storage.index_store import SimpleIndexStore\n",
            "from llama_index.core import Document, Settings\n",
            "\n",
            "import uuid # do we really need it here?\n",
            "import logging\n",
            "\n",
            "# Setup NebiusEmbedding model with custom_http_client\n",
            "# model_name = \"BAAI/bge-en-icl\" #- setup in config section\n",
            "custom_http_client = httpx.Client(timeout=60.0)\n",
            "embedding_model = NebiusEmbedding(\n",
            "    api_key=os.environ.get(\"API_KEY\"),\n",
            "    model_name=model_name,\n",
            "    http_client=custom_http_client,\n",
            "    api_base=\"https://api.studio.nebius.ai/v1/\",  # Explicitly specify api_base\n",
            "    # api_base=api_base,  # Using api_base variable defined in the config section doesn't work - probably, due to some bugs in NebiusEmbedding module\n",
            ")\n",
            "\n",
            "# Determine dimension of embedding model - needed for FAISS index\n",
            "sample_text = \"test text to create an embedding\"\n",
            "sample_embedding = embedding_model.get_text_embedding(sample_text)\n",
            "embedding_dimension = len(sample_embedding)\n",
            "\n",
            "print(f\"\\nEmbedding Model {model_name} creates {embedding_dimension}-dimensional vectors.\\n\")\n",
            "print(f\"Sample embedding for text '{sample_text}':\")\n",
            "print(sample_embedding[:10], \"...\\n\")\n",
            "\n",
            "# Set the embedding model in LlamaIndex settings\n",
            "Settings.embed_model = embedding_model\n",
            "\n",
            "def create_or_update_faiss_index(\n",
            "    documents: list[Document], index_path: Path, embedding_model: NebiusEmbedding, embedding_dimension: int\n",
            ") -> None:\n",
            "    \"\"\"\n",
            "    Creates or updates a FAISS-based index with the given documents.\n",
            "\n",
            "    Args:\n",
            "        documents (list[Document]): List of Document objects to add to the index.\n",
            "        index_path (Path): Path to the directory where the index will be stored.\n",
            "        embedding_model (NebiusEmbedding): The embedding model used for indexing.\n",
            "        embedding_dimension (int): The dimension of the embeddings.\n",
            "\n",
            "    Returns:\n",
            "        None\n",
            "    \"\"\"\n",
            "    if documents:\n",
            "        newly_added = len(documents)\n",
            "        if (index_path / \"default__vector_store.faiss\").exists():\n",
            "            # Load existing index\n",
            "            vector_store = FaissVectorStore.from_persist_path(str(index_path / \"default__vector_store.faiss\"))\n",
            "            storage_context = StorageContext.from_defaults(vector_store=vector_store, persist_dir=str(index_path))  # Reconstruct storage_context from files\n",
            "            index = load_index_from_storage(storage_context, embedding=embedding_model)  # Reloading the index\n",
            "\n",
            "            # Count vectors BEFORE\n",
            "            before_count = vector_store.client.ntotal\n",
            "\n",
            "            # Update index with new documents\n",
            "            for doc in documents:\n",
            "                index.insert(doc)  # Update index with new documents\n",
            "\n",
            "            # Count vectors AFTER\n",
            "            after_count = vector_store.client.ntotal\n",
            "\n",
            "            #logging.info(\"Inserted new documents into existing FAISS index.\")\n",
            "            logging.info(f\"Inserted {newly_added} new docs. Vectors before: {before_count}, after: {after_count}\")\n",
            "        else:\n",
            "            # Create a new index\n",
            "            vector_store = FaissVectorStore(faiss.IndexFlatIP(embedding_dimension))\n",
            "            storage_context = StorageContext.from_defaults(\n",
            "                vector_store=vector_store,\n",
            "                docstore=SimpleDocumentStore(),\n",
            "                index_store=SimpleIndexStore(),\n",
            "                persist_dir=str(index_path),\n",
            "            )\n",
            "            index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, embedding=embedding_model) # Create index from scratch\n",
            "\n",
            "            # Count vectors BEFORE\n",
            "            before_count = 0\n",
            "\n",
            "            # Count vectors AFTER\n",
            "            after_count = vector_store.client.ntotal\n",
            "\n",
            "            #logging.info(\"Inserted new documents into a freshly created FAISS index.\")\n",
            "            logging.info(\n",
            "                        f\"Created a new FAISS index and inserted {newly_added} docs. \"\n",
            "                        f\"Vectors before: 0, after: {after_count}\"\n",
            "                    )\n",
            "        # Persist the created/updated index\n",
            "        index.storage_context.persist(\n",
            "            persist_dir=str(index_path),\n",
            "            vector_store_fname=\"vector_store.faiss\",  # Important to save with .faiss suffix\n",
            "        )\n",
            "        logging.info(\"FAISS index created/updated and persisted successfully.\")\n",
            "\n",
            "    else:\n",
            "        logging.info(\"No documents to index for FAISS.\")\n",
            "\n",
            "# Execute FAISS indexing\n",
            "create_or_update_faiss_index(all_new_docs, faiss_index_path, embedding_model, embedding_dimension)\n",
            "\n",
            "\n",
            "# 12. Move processed files to a designated folder and update the records of processed files to keep track of which files have already been processed\n",
            "\n",
            "def move_files_to_processed(files: List[Path], destination_dir: Path) -> None:\n",
            "    \"\"\"\n",
            "    Moves processed files to the designated 'processed' directory.\n",
            "\n",
            "    Args:\n",
            "        files (List[Path]): List of file paths to move.\n",
            "        destination_dir (Path): Destination directory where files will be moved.\n",
            "\n",
            "    Returns:\n",
            "        None\n",
            "    \"\"\"\n",
            "    for file in files:\n",
            "        try:\n",
            "            target_path = destination_dir / file.relative_to(to_process_dir)  # Maintain folder structure\n",
            "            target_path.parent.mkdir(parents=True, exist_ok=True)  # Create parent folders if needed\n",
            "            file.rename(target_path)\n",
            "            logging.info(f\"Moved {file.name} to {target_path}\")\n",
            "        except Exception as e:\n",
            "            logging.error(f\"Failed to move file {file.name} to {target_path}: {e}\")\n",
            "\n",
            "def update_processed_files(record_path: Path, file_list: List[Path]) -> None:\n",
            "    \"\"\"\n",
            "    Updates the record of processed files (JSON files that keep track of files being processed).\n",
            "\n",
            "    Adds the names of the files that have just been processed to a JSON record.\n",
            "\n",
            "    Args:\n",
            "        record_path (Path): Path to the JSON record file.\n",
            "        file_list (List[Path]): List of file paths that have been processed.\n",
            "\n",
            "    Returns:\n",
            "        None\n",
            "    \"\"\"\n",
            "    try:\n",
            "        processed = set()\n",
            "        if record_path.exists():\n",
            "            with open(record_path, 'r') as f:\n",
            "                processed = set(json.load(f))\n",
            "        processed.update([file.name for file in file_list])\n",
            "        with open(record_path, 'w') as f:\n",
            "            json.dump(list(processed), f)\n",
            "        logging.info(f\"Updated processed files record at {record_path}.\")\n",
            "    except Exception as e:\n",
            "        logging.error(f\"Error updating processed files record at {record_path}: {e}\")\n",
            "\n",
            "# Move each type of processed files to /processed/\n",
            "move_files_to_processed(new_files_pdf, processed_dir)\n",
            "move_files_to_processed(new_files_excel, processed_dir)\n",
            "move_files_to_processed(new_files_ppt, processed_dir)\n",
            "move_files_to_processed(new_files_doc, processed_dir)\n",
            "\n",
            "# Update the processed files record for each file type\n",
            "update_processed_files(processed_files_record_pdf, new_files_pdf)\n",
            "update_processed_files(processed_files_record_excel, new_files_excel)\n",
            "update_processed_files(processed_files_record_ppt, new_files_ppt)\n",
            "update_processed_files(processed_files_record_doc, new_files_doc)\n",
            "\n",
            "\n",
            "\n",
            "# 13. Display text chunks and metadata created\n",
            "\n",
            "import json\n",
            "import textwrap\n",
            "from typing import List\n",
            "from llama_index.core import Document\n",
            "\n",
            "total_new_docs = len(all_new_docs)\n",
            "print(f\"Total new documents created: {total_new_docs}\\n\")\n",
            "\n",
            "def display_entries(entries: List[Document], file_type: str, start: int = 0, num: int = 5, text_width: int = 100, truncate: bool = False, max_length: int = 1500):\n",
            "    \"\"\"\n",
            "    Displays specified entries of metadata for verification.\n",
            "    The 'text' field (LlamaIndex Document) is displayed in a wrapped and readable format.\n",
            "\n",
            "    Args:\n",
            "        entries (List[Document]): List of Document objects.\n",
            "        file_type (str): The type of documents (e.g., 'PDF', 'Excel', 'PPT', 'DOC').\n",
            "        start (int, optional): Starting index. Defaults to 0.\n",
            "        num (int, optional): Number of entries to display. Defaults to 5.\n",
            "        text_width (int, optional): Width for text wrapping. Defaults to 100.\n",
            "        truncate (bool, optional): Whether to truncate long texts. Defaults to False.\n",
            "        max_length (int, optional): Maximum length of text to display if truncating. Defaults to 1500.\n",
            "\n",
            "    Returns:\n",
            "        None\n",
            "    \"\"\"\n",
            "    print(f\"Displaying {num} Entries starting from index {start} in {file_type} Documents:\\n\")\n",
            "\n",
            "    end = start + num\n",
            "    for i, doc in enumerate(entries[start:end], start=start + 1):\n",
            "        print(f\"Entry {i}:\")\n",
            "        metadata = doc.metadata\n",
            "        print(json.dumps(metadata, indent=4))\n",
            "\n",
            "        text = doc.text\n",
            "        if text:\n",
            "            print(\"\\nText:\")\n",
            "            if truncate and len(text) > max_length:\n",
            "                displayed_text = text[:max_length] + \"...\\n[Text truncated]\"\n",
            "            else:\n",
            "                displayed_text = text\n",
            "            wrapped_text = textwrap.fill(displayed_text, width=text_width)\n",
            "            print(wrapped_text)\n",
            "\n",
            "        print(\"\\n\" + \"-\" * text_width + \"\\n\")\n",
            "\n",
            "# Display entries for each document type\n",
            "display_entries(pdf_docs, 'PDF')\n",
            "display_entries(excel_docs, 'Excel', start=0, num=5)\n",
            "display_entries(ppt_docs, 'PPT', start=0, num=5)\n",
            "display_entries(doc_docs, 'DOC', start=0, num=5)\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
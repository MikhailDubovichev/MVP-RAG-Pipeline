{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Executive Summary\n",
        "\n",
        "This Inference notebook represents the second stage of the RAG pipeline, relying on the assets produced by the Data Preparation step. Created on 09.12.2024, it reads the previously generated LlamaIndex artifacts—namely default__vector_store.faiss, docstore.json, graph_store.json, and index_store.json—from /data/faiss_index/ in Google Drive.\n",
        "\n",
        "The notebook orchestrates hybrid search functionality. First, it uses the Faiss index for vector-based similarity matching, retrieving chunks of text that semantically match the query. Next, it uses the Whoosh index for BM25-based keyword matching. These results are combined and subsequently reranked by a cross-encoder (e.g., a Sentence Transformers model). The final set of top-ranked chunks is then assembled into a “context” string fed into an LLM (here, a Nebius-hosted model).\n",
        "\n",
        "By running the user's query against these multiple retrieval layers, the pipeline maximizes the likelihood of returning accurate, contextually relevant information. After context assembly, the LLM is prompted to generate an answer and to optionally list the sources. This approach provides a transparent chain of evidence, showing how the final response was derived.\n",
        "\n",
        "Additionally, the notebook employs Gradio to create a simple chat interface where a user can type queries and see both an answer and the relevant reference snippets. This user-facing UI, along with robust logging, closes the loop on an effective knowledge retrieval and answering system.\n",
        "\n",
        "Inference pipeline takes as an input files, created in Data Preparation pipeline:\n",
        "FAISS\n",
        "1) default__vectore_store.faiss\n",
        "2) docstore.json\n",
        "3) graph_store.json\n",
        "4) index_store.json\n",
        "WHOOSH\n",
        "5) MAIN (Whoosh index)\n",
        "These files are native to StorageContent class of LlamaIndex and will be used in the downstream Inference pipeline."
      ],
      "metadata": {
        "id": "TIat_nmUHp8t"
      },
      "id": "TIat_nmUHp8t"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# History of updates:\n",
        "V08 - added configuration section, timestamps to logs, batching to reranking function and treshold to combined results function.\n",
        "\n",
        "V07 - nothing was done on inference side of the pipeline\n",
        "\n",
        "V06 - I added hybrid search (Vector search + Lexical search instead of just vector one), after that I added cross-encoder reranker. So now top-k documents by cross-encoder reranker go to an LLM prompt.\n",
        "\n",
        "V05 - LLM was incorporated as a generator. Before that we just used index.as_retriever - so, LLM wasn't anyhow deployed on the pipeline.\n",
        "\n",
        "V03 - I added functionality of setting up similarity treshold. So one can filter retireved chunks based on similarity score (cosine similarity). Here LlamaIndex abstractions also were very useful. I also fixed some bugs of index calling OpenAI embedding model instead of Nebius (I setup Nebius embedding model and LLM as Settings.models and also explicitly setup nebuis embedding models in index parameters).\n",
        "\n",
        "V02 - in fact, most of the updates of v02 were maid in upstream pipeline. Here I've just integrated upstream changes."
      ],
      "metadata": {
        "id": "cjfB18O9pZHY"
      },
      "id": "cjfB18O9pZHY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Backlog\n",
        "\n",
        "ISSUES TO ADDRESS:\n",
        "\n",
        "1) in lexical search - we return only file name as a source. No, row or page or slide as in vector search - we need to add this logic  \n",
        "2) no evaluation of my retriever: metrics like Hit Rate, Mean Reciprocal Rank (MRR), or Normalized Discounted Cumulative Gain (NDCG):\n",
        "3) So far, no prompt has been implemented (multy-shot prompts, for example) - an archtecture and prompts are to be choosen based on problems RAG are solving\n",
        "4) Caching Mechanism: Implement caching for frequently asked questions to reduce redundant computations and API calls.\n",
        "5) Security Measures: Ensure that user inputs are sanitized to prevent potential injection attacks, especially if integrating with other systems or databases."
      ],
      "metadata": {
        "id": "wOTd0de-pSkI"
      },
      "id": "wOTd0de-pSkI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1 Install required libraries"
      ],
      "metadata": {
        "id": "TaxyrO6peM7v"
      },
      "id": "TaxyrO6peM7v"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "70aa2619",
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "70aa2619",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "70110e18-514c-416f-cdbf-e1f7e4896cbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.59.9)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.13.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.7-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.6.0 (from gradio)\n",
            "  Downloading gradio_client-1.6.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.27.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.9.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.6.0->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.6.0->gradio) (14.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.13.1-py3-none-any.whl (57.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.6.0-py3-none-any.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.8/321.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.7-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.7 ffmpy-0.5.0 gradio-5.13.1 gradio-client-1.6.0 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.3 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.45.3 tomlkit-0.13.2 uvicorn-0.34.0\n",
            "Collecting llama-index\n",
            "  Downloading llama_index-0.12.14-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting llama-index-core\n",
            "  Downloading llama_index_core-0.12.14-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-parse\n",
            "  Downloading llama_parse-0.5.20-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting llama-index-readers-file\n",
            "  Downloading llama_index_readers_file-0.4.4-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.4.3-py3-none-any.whl.metadata (727 bytes)\n",
            "Collecting llama-index-cli<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_cli-0.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.4-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.3.14-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.4.2-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (3.11.11)\n",
            "Collecting dataclasses-json (from llama-index-core)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (1.2.15)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (2.10.5)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (9.0.0)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core)\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (4.12.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (1.17.2)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.11/dist-packages (from llama-parse) (8.1.8)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file) (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file) (2.2.2)\n",
            "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file)\n",
            "  Downloading pypdf-5.2.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.18.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file) (2.6)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.59.9)\n",
            "Collecting llama-cloud<0.2.0,>=0.1.8 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.11-py3-none-any.whl.metadata (912 bytes)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (3.1.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core)\n",
            "  Downloading marshmallow-3.26.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file) (2025.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core) (24.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file) (1.17.0)\n",
            "Downloading llama_index-0.12.14-py3-none-any.whl (6.9 kB)\n",
            "Downloading llama_index_core-0.12.14-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.5.20-py3-none-any.whl (16 kB)\n",
            "Downloading llama_index_readers_file-0.4.4-py3-none-any.whl (39 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_index_agent_openai-0.4.3-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.4.0-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.6.4-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_llms_openai-0.3.14-py3-none-any.whl (14 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.4.2-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading pypdf-5.2.0-py3-none-any.whl (298 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.7/298.7 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading llama_cloud-0.1.11-py3-none-any.whl (250 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.6/250.6 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: striprtf, filetype, dirtyjson, pypdf, mypy-extensions, marshmallow, typing-inspect, tiktoken, llama-cloud, dataclasses-json, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "Successfully installed dataclasses-json-0.6.7 dirtyjson-1.0.8 filetype-1.2.0 llama-cloud-0.1.11 llama-index-0.12.14 llama-index-agent-openai-0.4.3 llama-index-cli-0.4.0 llama-index-core-0.12.14 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.4 llama-index-llms-openai-0.3.14 llama-index-multi-modal-llms-openai-0.4.2 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.4 llama-index-readers-llama-parse-0.4.0 llama-parse-0.5.20 marshmallow-3.26.0 mypy-extensions-1.0.0 pypdf-5.2.0 striprtf-0.0.26 tiktoken-0.8.0 typing-inspect-0.9.0\n",
            "Collecting llama-index-embeddings-nebius\n",
            "  Downloading llama_index_embeddings_nebius-0.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting llama-index-llms-nebius\n",
            "  Downloading llama_index_llms_nebius-0.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-nebius) (0.12.14)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-nebius) (0.3.1)\n",
            "Collecting llama-index-llms-openai-like<0.4.0,>=0.3.1 (from llama-index-llms-nebius)\n",
            "  Downloading llama_index_llms_openai_like-0.3.3-py3-none-any.whl.metadata (751 bytes)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (3.11.11)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (1.2.15)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (2.10.5)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (9.0.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (1.17.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-openai<0.4.0,>=0.3.0->llama-index-embeddings-nebius) (1.59.9)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-openai-like<0.4.0,>=0.3.1->llama-index-llms-nebius) (0.3.14)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.37.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-openai-like<0.4.0,>=0.3.1->llama-index-llms-nebius) (4.47.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (1.18.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (2024.11.6)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.4.0,>=0.3.0->llama-index-embeddings-nebius) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.4.0,>=0.3.0->llama-index-embeddings-nebius) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.4.0,>=0.3.0->llama-index-embeddings-nebius) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.4.0,>=0.3.0->llama-index-embeddings-nebius) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (3.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.4.0,>=0.3.1->llama-index-llms-nebius) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.4.0,>=0.3.1->llama-index-llms-nebius) (0.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.4.0,>=0.3.1->llama-index-llms-nebius) (24.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.4.0,>=0.3.1->llama-index-llms-nebius) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.4.0,>=0.3.1->llama-index-llms-nebius) (0.5.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-nebius) (3.26.0)\n",
            "Downloading llama_index_embeddings_nebius-0.3.1-py3-none-any.whl (3.6 kB)\n",
            "Downloading llama_index_llms_nebius-0.1.1-py3-none-any.whl (2.7 kB)\n",
            "Downloading llama_index_llms_openai_like-0.3.3-py3-none-any.whl (3.1 kB)\n",
            "Installing collected packages: llama-index-llms-openai-like, llama-index-embeddings-nebius, llama-index-llms-nebius\n",
            "Successfully installed llama-index-embeddings-nebius-0.3.1 llama-index-llms-nebius-0.1.1 llama-index-llms-openai-like-0.3.3\n",
            "Collecting llama-index-vector-stores-faiss\n",
            "  Downloading llama_index_vector_stores_faiss-0.3.0-py3-none-any.whl.metadata (658 bytes)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-vector-stores-faiss) (0.12.14)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (3.11.11)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.2.15)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2.10.5)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (9.0.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.17.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.18.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (3.26.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (0.14.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (24.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.3.1)\n",
            "Downloading llama_index_vector_stores_faiss-0.3.0-py3-none-any.whl (3.9 kB)\n",
            "Installing collected packages: llama-index-vector-stores-faiss\n",
            "Successfully installed llama-index-vector-stores-faiss-0.3.0\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.9.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0.post1\n",
            "Collecting whoosh\n",
            "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl.metadata (3.1 kB)\n",
            "Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.8/468.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: whoosh\n",
            "Successfully installed whoosh-2.7.4\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.3.1)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.47.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.27.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
            "Downloading sentence_transformers-3.4.0-py3-none-any.whl (275 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 3.3.1\n",
            "    Uninstalling sentence-transformers-3.3.1:\n",
            "      Successfully uninstalled sentence-transformers-3.3.1\n",
            "Successfully installed sentence-transformers-3.4.0\n"
          ]
        }
      ],
      "source": [
        "# 1. Install required libraries\n",
        "!pip install openai gradio httpx\n",
        "!pip install llama-index llama-index-core llama-parse llama-index-readers-file\n",
        "!pip install llama-index-embeddings-nebius llama-index-llms-nebius\n",
        "!pip install llama-index-vector-stores-faiss\n",
        "!pip install faiss-cpu\n",
        "!pip install whoosh\n",
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2 Configuration: Mount Google Drive, Load API keys and Config Environment\n",
        "\n",
        "Mounts your Google Drive for access to the FAISS/Whoosh indexes and loads the API key for Nebius AI from a JSON config. Sets the environment for inference, including which LLM model to use and any threshold parameters (like reranking thresholds).\n"
      ],
      "metadata": {
        "id": "S-DEKP53dOTf"
      },
      "id": "S-DEKP53dOTf"
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Configuration: Mount Google Drive, Load API keys and config environment\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "import openai\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "\n",
        "# Define the data directory path\n",
        "data_directory = Path(\"/content/gdrive/MyDrive/RAG_Project_5/data/\")\n",
        "\n",
        "# Load the API key\n",
        "config_path = Path(\"/content/gdrive/MyDrive/Colab_Notebooks/config.json\")\n",
        "with open(config_path, encoding=\"utf-8-sig\") as config_file:\n",
        "    config = json.load(config_file)\n",
        "    os.environ[\"API_KEY\"] = config[\"API_KEY\"]\n",
        "\n",
        "# Set the API key and endpoint globally\n",
        "openai.api_key = os.environ[\"API_KEY\"]\n",
        "openai.api_base = \"https://api.studio.nebius.ai/v1/\"  # Nebius AI endpoint\n",
        "\n",
        "\n",
        "nebius_model=\"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n",
        "\n",
        "reranking_treshold = 0.2 # setting up a treshold for reranked retrieval results (should be setup manually)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVPUv7px9gdL",
        "outputId": "720d3a96-598b-4809-f9a0-14b4b42045c0"
      },
      "id": "DVPUv7px9gdL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Logging configuration\n",
        "\n",
        "Enables logging during inference with a rotating file handler to record user queries and pipeline activities. This helps monitor query handling, time taken for searches, and any issues with model responses or indexing lookups."
      ],
      "metadata": {
        "id": "JQSVqhzHWdna"
      },
      "id": "JQSVqhzHWdna"
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Setup Logging with log rotation\n",
        "\n",
        "import logging\n",
        "from logging.handlers import RotatingFileHandler\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "def setup_logging(\n",
        "    log_folder: Path,\n",
        "    log_file: str = \"inference_pipeline.log\",\n",
        "    max_bytes: int = 5 * 1024 * 1024,  # 5 MB\n",
        "    backup_count: int = 3,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Configures logging with log rotation to write logs to both a file and the console.\n",
        "\n",
        "    Args:\n",
        "        log_folder (Path): The directory where the log file will be stored.\n",
        "        log_file (str, optional): The name of the log file. Defaults to \"inference_pipeline.log\".\n",
        "        max_bytes (int, optional): Maximum size of the log file in bytes before it is rotated. Defaults to 5 MB.\n",
        "        backup_count (int, optional): Number of backup files to keep. Defaults to 3.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    os.makedirs(log_folder, exist_ok=True)  # Ensure the folder exists\n",
        "    log_file_path = log_folder / log_file\n",
        "\n",
        "    # Create a RotatingFileHandler\n",
        "    rotating_handler = RotatingFileHandler(\n",
        "        filename=log_file_path,\n",
        "        maxBytes=max_bytes,\n",
        "        backupCount=backup_count,\n",
        "        encoding=\"utf-8\",\n",
        "    )\n",
        "\n",
        "    # Configure logging\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "        handlers=[\n",
        "            rotating_handler,  # Handles log rotation\n",
        "            logging.StreamHandler(),  # Logs also appear in Colab's output\n",
        "        ],\n",
        "        force=True,  # Force the configuration to apply even if logging was already configured\n",
        "    )\n",
        "\n",
        "    logging.info(\"Logging with rotation has been successfully configured.\")\n",
        "\n",
        "# Initialize logging with log rotation\n",
        "setup_logging(\n",
        "    log_folder=Path(\"/content/gdrive/MyDrive/RAG_Project_5/inference/\"),\n",
        "    log_file=\"inference_pipeline.log\",\n",
        "    max_bytes=5 * 1024 * 1024,  # 5 MB\n",
        "    backup_count=3,  # Keep up to 3 backup files\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOVMLBN7-XUf",
        "outputId": "a677df79-af58-4d74-dffe-2e68ce605d8c"
      },
      "id": "MOVMLBN7-XUf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-14 17:35:05,596 - INFO - Logging with rotation has been successfully configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Load the LlamaIndex-based FAISS Index, Whoosh Index, Initialize LLM and Embedding Model\n",
        "\n",
        "Loads existing FAISS and Whoosh indexes from disk. Initializes the Nebius-based embedding and LLM (set as defaults in Settings) for both vector and language tasks. Checks for potential missing indexes to prevent query failures at inference."
      ],
      "metadata": {
        "id": "jLH0eEfhnMMg"
      },
      "id": "jLH0eEfhnMMg"
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Load the LlamaIndex-based FAISS Index, Whoosh Index, Initialize LLM and Embedding Model\n",
        "\n",
        "import httpx  # For custom HTTP client if needed\n",
        "from whoosh.index import open_dir\n",
        "from whoosh.qparser import QueryParser\n",
        "from llama_index.embeddings.nebius import NebiusEmbedding\n",
        "from llama_index.llms.nebius import NebiusLLM\n",
        "from llama_index.core import Settings, StorageContext, load_index_from_storage\n",
        "from llama_index.vector_stores.faiss import FaissVectorStore\n",
        "from pathlib import Path\n",
        "import logging\n",
        "\n",
        "# Setup NebiusEmbedding (same as in data preparation pipeline)\n",
        "custom_http_client = httpx.Client(timeout=60.0)  # Fighting a bug in NebiusEmbedding library. More details in data preparation pipeline\n",
        "embedding_model = NebiusEmbedding(\n",
        "    api_key=os.environ[\"API_KEY\"],\n",
        "    model_name=\"BAAI/bge-en-icl\",\n",
        "    http_client=custom_http_client,\n",
        "    api_base=\"https://api.studio.nebius.ai/v1/\"  # Explicitly specifying api_base!!! It took me 2 hours to debug!\n",
        "    # UPDATE: Check in the future if the Nebius class in the library is fixed, so custom_http_client and api_base may no longer be needed.\n",
        ")\n",
        "\n",
        "# Setup Nebius LLM\n",
        "llm = NebiusLLM(\n",
        "    api_key=os.environ[\"API_KEY\"],\n",
        "    model=\"meta-llama/Meta-Llama-3.1-405B-Instruct\", # You setup model name in configuration section - it should be setup here.\n",
        "    temperature=0.2  # Set low temperature since we're dealing with engineering data and don't need too much creativity.\n",
        ")\n",
        "\n",
        "# Setup Nebius models as default\n",
        "# Setting up Nebius models as default early ensures methods using the Embedding Model don't default to LlamaIndex's native OpenAI models (avoids conflicts).\n",
        "Settings.embed_model = embedding_model\n",
        "Settings.llm_model = llm\n",
        "\n",
        "# FAISS Index Path\n",
        "faiss_index_path = Path(data_directory/'faiss_index')\n",
        "\n",
        "if (faiss_index_path / \"index_store.json\").exists():\n",
        "    # Load the FAISS vector store from the .faiss file\n",
        "    vector_store = FaissVectorStore.from_persist_path(str(faiss_index_path / \"default__vector_store.faiss\"))  # Note double underscore `__` in `default__vector_store.faiss`.\n",
        "    # Create a StorageContext with the vector_store\n",
        "    storage_context = StorageContext.from_defaults(\n",
        "        persist_dir=str(faiss_index_path),\n",
        "        vector_store=vector_store,\n",
        "    )\n",
        "    # Load the FAISS index from storage\n",
        "    faiss_index = load_index_from_storage(storage_context, embedding=embedding_model)\n",
        "    logging.info(\"FAISS index loaded successfully (LlamaIndex).\")\n",
        "else:\n",
        "    logging.error(f\"LlamaIndex FAISS index not found at {faiss_index_path}. Ensure the Data Preparation pipeline has been run.\")\n",
        "    raise FileNotFoundError(f\"No LlamaIndex index found at {faiss_index_path}.\")\n",
        "\n",
        "# Whoosh Index Path\n",
        "whoosh_index_path = Path(data_directory/'whoosh_index')\n",
        "\n",
        "if whoosh_index_path.exists():\n",
        "    # Load the Whoosh index\n",
        "    whoosh_index = open_dir(str(whoosh_index_path))\n",
        "    logging.info(\"Whoosh index loaded successfully.\")\n",
        "else:\n",
        "    logging.error(f\"Whoosh index not found at {whoosh_index_path}. Ensure the Data Preparation pipeline has been run.\")\n",
        "    raise FileNotFoundError(f\"No Whoosh index found at {whoosh_index_path}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akF918n6-CC9",
        "outputId": "e33a4ae9-fdee-4229-a634-dd73569de301"
      },
      "id": "akF918n6-CC9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-14 17:35:25,766 - INFO - NumExpr defaulting to 2 threads.\n",
            "2025-01-14 17:35:45,396 - INFO - Loading faiss with AVX2 support.\n",
            "2025-01-14 17:35:45,765 - INFO - Successfully loaded faiss with AVX2 support.\n",
            "2025-01-14 17:35:45,785 - INFO - Loading llama_index.vector_stores.faiss.base from /content/gdrive/MyDrive/RAG_Project_5/data/faiss_index/default__vector_store.faiss.\n",
            "2025-01-14 17:35:47,010 - INFO - Loading all indices.\n",
            "2025-01-14 17:35:47,270 - INFO - FAISS index loaded successfully (LlamaIndex).\n",
            "2025-01-14 17:35:47,594 - INFO - Whoosh index loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5 Define Hybrid Search Logic\n",
        "\n",
        "Implements a hybrid retrieval mechanism: FAISS-based semantic/vector search combined with Whoosh-based BM25 for text-based matching. Afterwards, a cross-encoder model reranks the combined results. This step ensures relevant documents are extracted from multiple angles (semantic similarity + text matching) and then refined by the cross-encoder for final relevance ordering."
      ],
      "metadata": {
        "id": "93D1Y5f3aMlp"
      },
      "id": "93D1Y5f3aMlp"
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 Define Hybrid Search Logic\n",
        "\n",
        "import numpy as np\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "def perform_vector_search(query: str, top_k: int = 10) -> list:\n",
        "    \"\"\"\n",
        "    Retrieve top-k results from FAISS vector index, including metadata.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's query.\n",
        "        top_k (int, optional): Number of top results to retrieve. Defaults to 10.\n",
        "\n",
        "    Returns:\n",
        "        list: List of tuples containing (text, score, metadata).\n",
        "    \"\"\"\n",
        "    retriever = faiss_index.as_retriever(similarity_top_k=top_k)\n",
        "    results = retriever.retrieve(query)\n",
        "    # Return (text, score, metadata)\n",
        "    return [(res.node.text, res.score if res.score is not None else 0.0, res.node.metadata) for res in results]\n",
        "\n",
        "def perform_bm25_search(query: str, top_k: int = 10) -> list:\n",
        "    \"\"\"\n",
        "    Retrieve top-k results from Whoosh BM25 search, including metadata.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's query.\n",
        "        top_k (int, optional): Number of top results to retrieve. Defaults to 10.\n",
        "\n",
        "    Returns:\n",
        "        list: List of tuples containing (text, score, metadata).\n",
        "    \"\"\"\n",
        "    with whoosh_index.searcher() as searcher:\n",
        "        parser = QueryParser(\"content\", schema=whoosh_index.schema)\n",
        "        try:\n",
        "            parsed_query = parser.parse(query)\n",
        "            results = searcher.search(parsed_query, limit=top_k)\n",
        "            bm25_results = []\n",
        "            for hit in results:\n",
        "                content = hit[\"content\"]\n",
        "                score = hit.score\n",
        "                metadata = {\n",
        "                    \"source\": hit.get(\"source\", \"Unknown Source\"),\n",
        "                    \"sheet\": hit.get(\"sheet\", None),\n",
        "                    \"row_number\": hit.get(\"row_number\", None),\n",
        "                    \"slide_number\": hit.get(\"slide_number\", None),\n",
        "                    \"section\": hit.get(\"section\", None),\n",
        "                    \"chunk_number\": hit.get(\"chunk_number\", None),\n",
        "                    \"total_chunks_in_section\": hit.get(\"total_chunks_in_section\", None),\n",
        "                    \"page_number\": hit.get(\"page_number\", None),\n",
        "                    \"doc_id\": hit.get(\"doc_id\", None),\n",
        "                    # Add other metadata fields as needed\n",
        "                }\n",
        "                bm25_results.append((content, score, metadata))\n",
        "            logging.info(f\"BM25 search results: {bm25_results}\")\n",
        "            # Return (text, score, metadata)\n",
        "            return bm25_results\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Whoosh search error: {e}\")\n",
        "            return []\n",
        "\n",
        "def combine_results(vector_results, bm25_results, alpha: float = 0.5) -> list:\n",
        "    \"\"\"\n",
        "    Combines FAISS and Whoosh results using Reciprocal Rank Fusion.\n",
        "\n",
        "    Args:\n",
        "        vector_results (list): List of tuples (text, score, metadata) from FAISS.\n",
        "        bm25_results (list): List of tuples (text, score, metadata) from Whoosh.\n",
        "        alpha (float, optional): Weight for FAISS results. Defaults to 0.5.\n",
        "\n",
        "    Returns:\n",
        "        list: Combined list of tuples (text, combined_score, metadata).\n",
        "    \"\"\"\n",
        "    combined_scores = {}\n",
        "    metadata_mapping = {}\n",
        "\n",
        "    # Process FAISS results\n",
        "    for idx, (text, score, metadata) in enumerate(vector_results):\n",
        "        if text:\n",
        "            combined_scores[text] = combined_scores.get(text, 0) + alpha / (idx + 1)\n",
        "            metadata_mapping[text] = metadata  # Store metadata from FAISS\n",
        "\n",
        "    # Process BM25 results\n",
        "    for idx, (text, score, metadata) in enumerate(bm25_results):\n",
        "        if text:\n",
        "            combined_scores[text] = combined_scores.get(text, 0) + (1 - alpha) / (idx + 1)\n",
        "            if text not in metadata_mapping:\n",
        "                metadata_mapping[text] = metadata  # Store metadata from Whoosh if not already present\n",
        "\n",
        "    # Combine scores and retrieve metadata\n",
        "    combined_results = [(text, score, metadata_mapping[text]) for text, score in combined_scores.items()]\n",
        "\n",
        "    # Sort results by combined score in descending order\n",
        "    sorted_results = sorted(combined_results, key=lambda x: x[1], reverse=True)\n",
        "    return sorted_results\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Load a pretrained cross-encoder model\n",
        "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-TinyBERT-L-6\")\n",
        "\n",
        "def rerank_with_cross_encoder(query: str, documents: list, top_k: int = 10, batch_size: int = 16) -> list:\n",
        "    \"\"\"\n",
        "    Rerank documents using a cross-encoder model with batching.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's query.\n",
        "        documents (list): List of tuples (text, combined_score, metadata).\n",
        "        top_k (int, optional): Number of top documents to return after reranking. Defaults to 10.\n",
        "        batch_size (int, optional): Number of document-query pairs to process in a batch. Defaults to 16.\n",
        "\n",
        "    Returns:\n",
        "        list: Reranked list of top-k documents with their scores and metadata.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Prepare input pairs for the cross-encoder\n",
        "        input_pairs = [(query, doc[0]) for doc in documents]  # doc[0] is the document text\n",
        "        scores = []\n",
        "\n",
        "        # Process input pairs in batches\n",
        "        for i in range(0, len(input_pairs), batch_size):\n",
        "            batch = input_pairs[i:i + batch_size]\n",
        "            try:\n",
        "                batch_scores = cross_encoder.predict(batch)  # Predict scores for the batch\n",
        "                scores.extend(batch_scores)\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Cross-encoder prediction failed for batch {i // batch_size}: {e}\")\n",
        "                scores.extend([0.0] * len(batch))  # Fallback to zero scores for this batch\n",
        "\n",
        "        # Combine scores with documents\n",
        "        scored_docs = [(doc[0], score, doc[2]) for doc, score in zip(documents, scores)]\n",
        "\n",
        "        # Sort documents by relevance score in descending order\n",
        "        reranked_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Filter by relevance threshold\n",
        "        filtered_reranked_docs = [doc for doc in reranked_docs if doc[1] >= reranking_treshold]\n",
        "\n",
        "        return filtered_reranked_docs[:top_k]  # Return documents from top-k, filtered by threshold\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Unexpected error in rerank_with_cross_encoder: {e}\")\n",
        "        return []  # Return an empty list in case of failure\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318,
          "referenced_widgets": [
            "5c5b410b01dd434284ffa4c10171bc2a",
            "852f73b1aff14d8f8c4ade66af1dfa7d",
            "0bb25efedcd3493bacfa530c80e48586",
            "64d4a5d70e034825b888dc7f6b33dd6b",
            "47a82eada50d4b67b477148ddf037708",
            "3d22c7e1a3b649889f22d3fabeee635b",
            "6d266f77df4e48d097daeb93ef54e5fa",
            "fe33edd9056c443892c33bbbd50a6b43",
            "2364ab7d4ae04c13b80956548c8e7a28",
            "b633679d7f5f476f93f50beb5b94587a",
            "bc6c0664680541599c9e878a60f5f475",
            "fb1b724aabfb43e7bf7592f741799a13",
            "e3ec32bd869a44569891c802f321829e",
            "966905f594234002852196f1a886239b",
            "474e1a2f3ce54d1ca29457e6a18ffc84",
            "70b1c5325ba648a0a094be4ef8ba0a26",
            "e8ceb1fa29f94200be0629f948c1c75b",
            "48a0e4ba5fc240088ea4c613e3d7587e",
            "818910630c0a4ce793a64addf4a7b875",
            "40c8531279e44c669f90683671f6b700",
            "1d8122f85b0245cca568bee2ca629979",
            "b41c54effdaf45769a86e23624ae5d89",
            "e96a9b5637bd42babe82ad869a27af1d",
            "66319c1604f84a1e9699c81b9641968f",
            "c54d395c2bf949bfaddb33c0e0662434",
            "6d65720983c94e6b9a3790004ae7f88f",
            "1a369e552b154b148c4c94fc52fd64f7",
            "6fdd8c8b56594051a7414a3ac7955952",
            "bf86487270c0437d838f639b1e985193",
            "f81dd31b7d364bf79ccb81e53a5af9aa",
            "78ea5891bf724e1192302822e09e38d2",
            "5567201bef6c42fea4c71684367d9a48",
            "dd771157bcb5420db68dba5da011a7f3",
            "f1b8e23ce7c0463e80e2f76236e8c70e",
            "9e5d70e6934e4833a61a71b84760f1f0",
            "584ed65d62af43898f24eef4a18a936f",
            "76b7b94516ec46aea09b40593c22c491",
            "36acd112da7a4b3b83bd83f8aa60f099",
            "6718465be6e344d3a2ce3e48dbddcf6e",
            "9e79e7dd68c84d0b91717dca58a46b4a",
            "06568652bb334757b25392001cf85d68",
            "44dadd41b1c54d398e289d935e780974",
            "fe80e5b382b6424baf7662ebcbeb8b06",
            "ad105995fbf74658a1a5c6ef92f7aece",
            "31c40bf81840437fbd16c924e441a26a",
            "9bad7a2a4dec4e5da12be0479f5d10bf",
            "c044251d70bb4bd986c1fb4e8406166f",
            "1ca72ef741df4da1a86978e1c500bef8",
            "c4b6d97fc7e748bdbcdf524cab3158f6",
            "4aaea26fe4dc4fe9a4594e1e46b05db8",
            "72ad734a58a44a5c9a2ed29a05b92832",
            "8ee46df6f5444d059b23c5b0651fec6f",
            "10950058258043ad9b76fe87ca751317",
            "79a03fcd048042598f97d3c63adffe0a",
            "435cb06c587f472f9eb99ddb65b2e8a0"
          ]
        },
        "id": "h_n4R7N7El1K",
        "outputId": "70e466a4-7e5a-43f2-9104-b57de65e5cac"
      },
      "id": "h_n4R7N7El1K",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c5b410b01dd434284ffa4c10171bc2a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb1b724aabfb43e7bf7592f741799a13"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/541 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e96a9b5637bd42babe82ad869a27af1d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1b8e23ce7c0463e80e2f76236e8c70e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31c40bf81840437fbd16c924e441a26a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-14 17:36:04,696 - INFO - Use pytorch device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 Create a LLM-based Retriever based on Hybrid Search Logic\n",
        "\n",
        "Implements a retrieval pipeline that uses both vector search and BM25, then reranks with a cross-encoder. Once the top results are determined, the system assembles a context for the language model. The LLM uses that context to produce a final answer. This function returns both the updated chat history and the system response.\n"
      ],
      "metadata": {
        "id": "8Lc00HeHII4D"
      },
      "id": "8Lc00HeHII4D"
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 Create a LLM-based Retriever based on Hybrid Search Logic\n",
        "\n",
        "import time  # Import time for timestamping\n",
        "\n",
        "def generate_llm_response(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Generates a response from the LLM based on the given prompt.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The input prompt for the LLM.\n",
        "\n",
        "    Returns:\n",
        "        str: The response generated by the LLM.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = llm.complete(prompt=prompt)\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"LLM generation failed: {e}\")\n",
        "        return \"An error occurred while generating the response. Please try again later.\"\n",
        "\n",
        "def get_answer(query: str, top_k: int = 10, alpha: float = 0.5, history=None):\n",
        "    \"\"\"\n",
        "    Retrieves answers for a given query using FAISS, Whoosh indices, and a Cross-Encoder Reranker.\n",
        "    Enhances the references with metadata attributes from both FAISS and Whoosh.\n",
        "\n",
        "    Logs the time taken for each significant step.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        start_time = time.time()  # Start timing\n",
        "        logging.info(\"Pipeline started.\")\n",
        "\n",
        "        if not query.strip():\n",
        "            logging.warning(\"Received an empty query.\")\n",
        "            return history, \"Please enter a valid question.\"\n",
        "\n",
        "        if history is None:\n",
        "            history = []\n",
        "\n",
        "        # Step 1: Perform FAISS Vector Search\n",
        "        vector_search_start = time.time()\n",
        "        vector_results = perform_vector_search(query, top_k * 2)\n",
        "        vector_search_time = time.time() - vector_search_start\n",
        "        logging.info(f\"Vector search completed in {vector_search_time:.2f} seconds.\")\n",
        "\n",
        "        # Step 2: Perform BM25 Search\n",
        "        bm25_search_start = time.time()\n",
        "        bm25_results = perform_bm25_search(query, top_k * 2)\n",
        "        bm25_search_time = time.time() - bm25_search_start\n",
        "        logging.info(f\"BM25 search completed in {bm25_search_time:.2f} seconds.\")\n",
        "\n",
        "        # Step 3: Combine Results\n",
        "        combine_results_start = time.time()\n",
        "        hybrid_results = combine_results(vector_results, bm25_results, alpha)\n",
        "        combine_results_time = time.time() - combine_results_start\n",
        "        logging.info(f\"Combining results completed in {combine_results_time:.2f} seconds.\")\n",
        "\n",
        "        # Step 4: Rerank with Cross-Encoder\n",
        "        rerank_start = time.time()\n",
        "        reranked_results = rerank_with_cross_encoder(query, hybrid_results, top_k=top_k, batch_size=top_k)  # Batch_size is also equal top_k\n",
        "        rerank_time = time.time() - rerank_start\n",
        "        logging.info(f\"Reranking completed in {rerank_time:.2f} seconds.\")\n",
        "\n",
        "        # Step 5: Prepare Context for LLM\n",
        "        context_start = time.time()\n",
        "        context = \"\\n\\n\".join([text for text, score, metadata in reranked_results])\n",
        "        context_time = time.time() - context_start\n",
        "        logging.info(f\"Context preparation completed in {context_time:.2f} seconds.\")\n",
        "\n",
        "        # Step 6: Generate LLM Response\n",
        "        llm_start = time.time()\n",
        "        llm_prompt = (\n",
        "            f\"Question: {query}\\n\\n\"\n",
        "            f\"Context:\\n{context}\\n\\n\"\n",
        "            \"Based on the provided context, is there enough relevant information to answer the question? \"\n",
        "            \"Respond with 'Yes' or 'No'. If yes, provide the answer; otherwise, say 'No relevant information available.'\"\n",
        "        )\n",
        "        llm_response = generate_llm_response(llm_prompt)\n",
        "        llm_time = time.time() - llm_start\n",
        "        logging.info(f\"LLM response generated in {llm_time:.2f} seconds.\")\n",
        "\n",
        "        # Step 7: Format Final Response\n",
        "        response_format_start = time.time()\n",
        "        references = []\n",
        "        for text, score, metadata in reranked_results:\n",
        "            if metadata:\n",
        "                source = metadata.get(\"source\", \"Unknown Source\")\n",
        "                sheet = f\"Sheet: {metadata['sheet']}\" if metadata.get(\"sheet\") else \"\"\n",
        "                row = f\"Row: {metadata['row_number']}\" if metadata.get(\"row_number\") else \"\"\n",
        "                slide = f\"Slide: {metadata['slide_number']}\" if metadata.get(\"slide_number\") else \"\"\n",
        "                section = f\"Section: {metadata['section']}\" if metadata.get(\"section\") else \"\"\n",
        "                page = f\"Page: {metadata['page_number']}\" if metadata.get(\"page_number\") else \"\"\n",
        "                chunk = f\"Chunk: {metadata['chunk_number']}\" if metadata.get(\"chunk_number\") else \"\"\n",
        "                total_chunks = f\"Total Chunks: {metadata['total_chunks_in_section']}\" if metadata.get(\"total_chunks_in_section\") else \"\"\n",
        "\n",
        "                # Combine all available metadata\n",
        "                metadata_str = \", \".join(filter(None, [source, sheet, row, slide, section, page, chunk, total_chunks]))\n",
        "                references.append(f\"{metadata_str}: {text[:50]}...\")\n",
        "            else:\n",
        "                references.append(f\"{text[:50]}...\")\n",
        "\n",
        "        references_text = \"\\n\".join([f\"- {ref}\" for ref in references])\n",
        "        if llm_response == 'No relevant information available.':\n",
        "            final_answer = llm_response\n",
        "        else:\n",
        "            final_answer = f\"{llm_response}\\n\\nSources:\\n{references_text}\"\n",
        "\n",
        "        response_format_time = time.time() - response_format_start\n",
        "        logging.info(f\"Response formatting completed in {response_format_time:.2f} seconds.\")\n",
        "\n",
        "        # Update chat history\n",
        "        history.append((query, final_answer))\n",
        "        total_time = time.time() - start_time\n",
        "        logging.info(f\"Pipeline completed in {total_time:.2f} seconds.\")\n",
        "\n",
        "        return history, history\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Unexpected Error in get_answer: {e}\")\n",
        "        response = \"An unexpected error occurred. Please try again later.\"\n",
        "        if history is not None:\n",
        "            history.append((query, response))\n",
        "        return history, history"
      ],
      "metadata": {
        "id": "iJyGlCcO4rQ6"
      },
      "id": "iJyGlCcO4rQ6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Building User Interface with Gradio\n",
        "\n",
        "Creates a simple Gradio interface with a text input for user queries and a chatbot-like response area. Users can type questions, and the hybrid search pipeline plus LLM response is triggered. The interface displays the final answer, along with any references used. Additionally, there’s a placeholder “Settings” tab for future configuration options."
      ],
      "metadata": {
        "id": "QMFDa27In6ua"
      },
      "id": "QMFDa27In6ua"
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Build Gradio Interface\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def chatbot_interface(user_input, history):\n",
        "    \"\"\"\n",
        "    Handles the Gradio interface interaction by invoking the get_answer function.\n",
        "\n",
        "    Args:\n",
        "        user_input (str): The user's input query.\n",
        "        history (list): The chat history (state).\n",
        "\n",
        "    Returns:\n",
        "        tuple: Updated history and the LLM response.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(f\"Received user input: {user_input}\")\n",
        "        if not user_input.strip():\n",
        "            return history, \"Please enter a valid query.\"\n",
        "\n",
        "        # Call the get_answer function\n",
        "        updated_history, response = get_answer(user_input, history=history)\n",
        "\n",
        "        # Log the response for debugging\n",
        "        logging.info(f\"LLM Response: {response}\")\n",
        "        return updated_history, response\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in chatbot_interface: {e}\")\n",
        "        return history, \"An unexpected error occurred. Please try again later.\"\n",
        "\n",
        "# Initialize Gradio Blocks\n",
        "with gr.Blocks(css=\"\"\"\n",
        "    #user-input {\n",
        "        padding-top: 2px !important;\n",
        "        font-size: 24px !important;\n",
        "        line-height: 1.2 !important;\n",
        "        background-color: #f9f9f9 !important;\n",
        "    } \"\"\") as demo:\n",
        "\n",
        "    gr.Markdown(\"### RAG Chatbot for Oil & Gas Drilling Engineers\")\n",
        "\n",
        "    # Chatbot Tab\n",
        "    with gr.Tab(\"Chatbot\"):\n",
        "        with gr.Row():\n",
        "            chatbot = gr.Chatbot(label=\"Chat Window\", show_label=True)\n",
        "        with gr.Row(equal_height=True):\n",
        "            user_input = gr.Textbox(\n",
        "                lines=2,\n",
        "                placeholder=\"Enter your question here...\",\n",
        "                label=\"Your Message\",\n",
        "                elem_id=\"user-input\"\n",
        "            )\n",
        "            send_button = gr.Button(\"Send\", scale=0.25)\n",
        "        state = gr.State(value=[])  # Initialize state as an empty list\n",
        "\n",
        "        # Define interaction logic\n",
        "        send_button.click(\n",
        "            fn=chatbot_interface,\n",
        "            inputs=[user_input, state],\n",
        "            outputs=[chatbot, state]\n",
        "        )\n",
        "\n",
        "    # Settings Tab (Future Enhancements Placeholder)\n",
        "    with gr.Tab(\"Settings\"):\n",
        "        gr.Markdown(\"### Settings\")\n",
        "        gr.Markdown(\"Settings can be configured here in the future.\")\n",
        "\n",
        "    # Footer\n",
        "    gr.Markdown(\"© Noname Company\")\n",
        "\n",
        "# Launch Gradio interface\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837
        },
        "id": "BI_O1asuhwjk",
        "outputId": "b0dfaf8f-b1fc-4720-edf8-350612948f28"
      },
      "id": "BI_O1asuhwjk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-14 17:36:15,125 - INFO - HTTP Request: GET https://api.gradio.app/gradio-messaging/en \"HTTP/1.1 200 OK\"\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:273: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/base.py:201: UserWarning: 'scale' value should be an integer. Using 0.25 will cause issues.\n",
            "  warnings.warn(\n",
            "2025-01-14 17:36:17,176 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
            "2025-01-14 17:36:17,392 - INFO - HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
            "2025-01-14 17:36:17,444 - INFO - HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-14 17:36:17,700 - INFO - HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n",
            "2025-01-14 17:36:17,879 - INFO - HTTP Request: GET https://cdn-media.huggingface.co/frpc-gradio-0.3/frpc_linux_amd64 \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* Running on public URL: https://72575f3d2c8c9fcc66.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-14 17:36:18,550 - INFO - HTTP Request: HEAD https://72575f3d2c8c9fcc66.gradio.live \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://72575f3d2c8c9fcc66.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# APPENDIXES:"
      ],
      "metadata": {
        "id": "Tu1oXH0nS-yu"
      },
      "id": "Tu1oXH0nS-yu"
    },
    {
      "cell_type": "code",
      "source": [
        "# SKIP ME\n",
        "\n",
        "# APPENDIXES: Print all the codecells from the pipeline above (skips cells marked \"# SKIP ME\")\n",
        "\n",
        "import json\n",
        "\n",
        "# Fetch the current notebook's metadata and contents\n",
        "from google.colab import _message\n",
        "\n",
        "def get_notebook_content():\n",
        "    try:\n",
        "        notebook = _message.blocking_request('get_ipynb', timeout_sec=5)\n",
        "        return notebook['ipynb']\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching notebook content: {e}\")\n",
        "        return None\n",
        "\n",
        "# Get the notebook content\n",
        "notebook_data = get_notebook_content()\n",
        "\n",
        "if notebook_data:\n",
        "    code_cells = [\n",
        "        \"\".join(cell[\"source\"])\n",
        "        for cell in notebook_data.get(\"cells\", [])\n",
        "        if cell[\"cell_type\"] == \"code\" and \"# SKIP ME\" not in \"\".join(cell[\"source\"])\n",
        "    ]\n",
        "\n",
        "    # Join the code cells with three empty lines between them\n",
        "    readable_code = \"\\n\\n\\n\".join(code_cells)\n",
        "\n",
        "#    print(\"Extracted Code Cells:\\n\\n\\n\")\n",
        "    print(readable_code)\n",
        "else:\n",
        "    print(\"Could not fetch notebook content.\")"
      ],
      "metadata": {
        "id": "TPjTIIE9S72y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72519d38-c9a6-4d19-a957-a52e9d4208c1",
        "collapsed": true
      },
      "id": "TPjTIIE9S72y",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 1. Install required libraries\n",
            "!pip install openai gradio httpx\n",
            "!pip install llama-index llama-index-core llama-parse llama-index-readers-file\n",
            "!pip install llama-index-embeddings-nebius llama-index-llms-nebius\n",
            "!pip install llama-index-vector-stores-faiss\n",
            "!pip install faiss-cpu\n",
            "!pip install whoosh\n",
            "!pip install -U sentence-transformers\n",
            "\n",
            "\n",
            "# 2. Configuration: Mount Google Drive, Load API keys and config environment\n",
            "\n",
            "from google.colab import drive\n",
            "import os\n",
            "import json\n",
            "from pathlib import Path\n",
            "import openai\n",
            "\n",
            "# Mount Google Drive\n",
            "drive.mount(\"/content/gdrive\")\n",
            "\n",
            "# Define the data directory path\n",
            "data_directory = Path(\"/content/gdrive/MyDrive/RAG_Project_5/data/\")\n",
            "\n",
            "# Load the API key\n",
            "config_path = Path(\"/content/gdrive/MyDrive/Colab_Notebooks/config.json\")\n",
            "with open(config_path, encoding=\"utf-8-sig\") as config_file:\n",
            "    config = json.load(config_file)\n",
            "    os.environ[\"API_KEY\"] = config[\"API_KEY\"]\n",
            "\n",
            "# Set the API key and endpoint globally\n",
            "openai.api_key = os.environ[\"API_KEY\"]\n",
            "openai.api_base = \"https://api.studio.nebius.ai/v1/\"  # Nebius AI endpoint\n",
            "\n",
            "\n",
            "nebius_model=\"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n",
            "\n",
            "reranking_treshold = 0.2 # setting up a treshold for reranked retrieval results (should be setup manually)\n",
            "\n",
            "\n",
            "# 3. Setup Logging with log rotation\n",
            "\n",
            "import logging\n",
            "from logging.handlers import RotatingFileHandler\n",
            "from pathlib import Path\n",
            "import os\n",
            "\n",
            "def setup_logging(\n",
            "    log_folder: Path,\n",
            "    log_file: str = \"inference_pipeline.log\",\n",
            "    max_bytes: int = 5 * 1024 * 1024,  # 5 MB\n",
            "    backup_count: int = 3,\n",
            ") -> None:\n",
            "    \"\"\"\n",
            "    Configures logging with log rotation to write logs to both a file and the console.\n",
            "\n",
            "    Args:\n",
            "        log_folder (Path): The directory where the log file will be stored.\n",
            "        log_file (str, optional): The name of the log file. Defaults to \"inference_pipeline.log\".\n",
            "        max_bytes (int, optional): Maximum size of the log file in bytes before it is rotated. Defaults to 5 MB.\n",
            "        backup_count (int, optional): Number of backup files to keep. Defaults to 3.\n",
            "\n",
            "    Returns:\n",
            "        None\n",
            "    \"\"\"\n",
            "    os.makedirs(log_folder, exist_ok=True)  # Ensure the folder exists\n",
            "    log_file_path = log_folder / log_file\n",
            "\n",
            "    # Create a RotatingFileHandler\n",
            "    rotating_handler = RotatingFileHandler(\n",
            "        filename=log_file_path,\n",
            "        maxBytes=max_bytes,\n",
            "        backupCount=backup_count,\n",
            "        encoding=\"utf-8\",\n",
            "    )\n",
            "\n",
            "    # Configure logging\n",
            "    logging.basicConfig(\n",
            "        level=logging.INFO,\n",
            "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
            "        handlers=[\n",
            "            rotating_handler,  # Handles log rotation\n",
            "            logging.StreamHandler(),  # Logs also appear in Colab's output\n",
            "        ],\n",
            "        force=True,  # Force the configuration to apply even if logging was already configured\n",
            "    )\n",
            "\n",
            "    logging.info(\"Logging with rotation has been successfully configured.\")\n",
            "\n",
            "# Initialize logging with log rotation\n",
            "setup_logging(\n",
            "    log_folder=Path(\"/content/gdrive/MyDrive/RAG_Project_5/inference/\"),\n",
            "    log_file=\"inference_pipeline.log\",\n",
            "    max_bytes=5 * 1024 * 1024,  # 5 MB\n",
            "    backup_count=3,  # Keep up to 3 backup files\n",
            ")\n",
            "\n",
            "\n",
            "# 4. Load the LlamaIndex-based FAISS Index, Whoosh Index, Initialize LLM and Embedding Model\n",
            "\n",
            "import httpx  # For custom HTTP client if needed\n",
            "from whoosh.index import open_dir\n",
            "from whoosh.qparser import QueryParser\n",
            "from llama_index.embeddings.nebius import NebiusEmbedding\n",
            "from llama_index.llms.nebius import NebiusLLM\n",
            "from llama_index.core import Settings, StorageContext, load_index_from_storage\n",
            "from llama_index.vector_stores.faiss import FaissVectorStore\n",
            "from pathlib import Path\n",
            "import logging\n",
            "\n",
            "# Setup NebiusEmbedding (same as in data preparation pipeline)\n",
            "custom_http_client = httpx.Client(timeout=60.0)  # Fighting a bug in NebiusEmbedding library. More details in data preparation pipeline\n",
            "embedding_model = NebiusEmbedding(\n",
            "    api_key=os.environ[\"API_KEY\"],\n",
            "    model_name=\"BAAI/bge-en-icl\",\n",
            "    http_client=custom_http_client,\n",
            "    api_base=\"https://api.studio.nebius.ai/v1/\"  # Explicitly specifying api_base!!! It took me 2 hours to debug!\n",
            "    # UPDATE: Check in the future if the Nebius class in the library is fixed, so custom_http_client and api_base may no longer be needed.\n",
            ")\n",
            "\n",
            "# Setup Nebius LLM\n",
            "llm = NebiusLLM(\n",
            "    api_key=os.environ[\"API_KEY\"],\n",
            "    model=\"meta-llama/Meta-Llama-3.1-405B-Instruct\", # You setup model name in configuration section - it should be setup here.\n",
            "    temperature=0.2  # Set low temperature since we're dealing with engineering data and don't need too much creativity.\n",
            ")\n",
            "\n",
            "# Setup Nebius models as default\n",
            "# Setting up Nebius models as default early ensures methods using the Embedding Model don't default to LlamaIndex's native OpenAI models (avoids conflicts).\n",
            "Settings.embed_model = embedding_model\n",
            "Settings.llm_model = llm\n",
            "\n",
            "# FAISS Index Path\n",
            "faiss_index_path = Path(data_directory/'faiss_index')\n",
            "\n",
            "if (faiss_index_path / \"index_store.json\").exists():\n",
            "    # Load the FAISS vector store from the .faiss file\n",
            "    vector_store = FaissVectorStore.from_persist_path(str(faiss_index_path / \"default__vector_store.faiss\"))  # Note double underscore `__` in `default__vector_store.faiss`.\n",
            "    # Create a StorageContext with the vector_store\n",
            "    storage_context = StorageContext.from_defaults(\n",
            "        persist_dir=str(faiss_index_path),\n",
            "        vector_store=vector_store,\n",
            "    )\n",
            "    # Load the FAISS index from storage\n",
            "    faiss_index = load_index_from_storage(storage_context, embedding=embedding_model)\n",
            "    logging.info(\"FAISS index loaded successfully (LlamaIndex).\")\n",
            "else:\n",
            "    logging.error(f\"LlamaIndex FAISS index not found at {faiss_index_path}. Ensure the Data Preparation pipeline has been run.\")\n",
            "    raise FileNotFoundError(f\"No LlamaIndex index found at {faiss_index_path}.\")\n",
            "\n",
            "# Whoosh Index Path\n",
            "whoosh_index_path = Path(data_directory/'whoosh_index')\n",
            "\n",
            "if whoosh_index_path.exists():\n",
            "    # Load the Whoosh index\n",
            "    whoosh_index = open_dir(str(whoosh_index_path))\n",
            "    logging.info(\"Whoosh index loaded successfully.\")\n",
            "else:\n",
            "    logging.error(f\"Whoosh index not found at {whoosh_index_path}. Ensure the Data Preparation pipeline has been run.\")\n",
            "    raise FileNotFoundError(f\"No Whoosh index found at {whoosh_index_path}.\")\n",
            "\n",
            "\n",
            "\n",
            "# 5 Define Hybrid Search Logic\n",
            "\n",
            "import numpy as np\n",
            "from sentence_transformers import CrossEncoder\n",
            "\n",
            "def perform_vector_search(query: str, top_k: int = 10):\n",
            "    \"\"\"\n",
            "    Retrieve top-k results from FAISS vector index, including metadata.\n",
            "\n",
            "    Args:\n",
            "        query (str): The user's query.\n",
            "        top_k (int, optional): Number of top results to retrieve. Defaults to 10.\n",
            "\n",
            "    Returns:\n",
            "        list: List of tuples containing (text, score, metadata).\n",
            "    \"\"\"\n",
            "    retriever = faiss_index.as_retriever(similarity_top_k=top_k)\n",
            "    results = retriever.retrieve(query)\n",
            "    # Return (text, score, metadata)\n",
            "    return [(res.node.text, res.score if res.score is not None else 0.0, res.node.metadata) for res in results]\n",
            "\n",
            "def perform_bm25_search(query: str, top_k: int = 10):\n",
            "    \"\"\"\n",
            "    Retrieve top-k results from Whoosh BM25 search, including metadata.\n",
            "\n",
            "    Args:\n",
            "        query (str): The user's query.\n",
            "        top_k (int, optional): Number of top results to retrieve. Defaults to 10.\n",
            "\n",
            "    Returns:\n",
            "        list: List of tuples containing (text, score, metadata).\n",
            "    \"\"\"\n",
            "    with whoosh_index.searcher() as searcher:\n",
            "        parser = QueryParser(\"content\", schema=whoosh_index.schema)\n",
            "        try:\n",
            "            parsed_query = parser.parse(query)\n",
            "            results = searcher.search(parsed_query, limit=top_k)\n",
            "            bm25_results = []\n",
            "            for hit in results:\n",
            "                content = hit[\"content\"]\n",
            "                score = hit.score\n",
            "                metadata = {\n",
            "                    \"source\": hit.get(\"source\", \"Unknown Source\"),\n",
            "                    \"sheet\": hit.get(\"sheet\", None),\n",
            "                    \"row_number\": hit.get(\"row_number\", None),\n",
            "                    \"slide_number\": hit.get(\"slide_number\", None),\n",
            "                    \"section\": hit.get(\"section\", None),\n",
            "                    \"chunk_number\": hit.get(\"chunk_number\", None),\n",
            "                    \"total_chunks_in_section\": hit.get(\"total_chunks_in_section\", None),\n",
            "                    \"page_number\": hit.get(\"page_number\", None),\n",
            "                    \"doc_id\": hit.get(\"doc_id\", None),\n",
            "                    # Add other metadata fields as needed\n",
            "                }\n",
            "                bm25_results.append((content, score, metadata))\n",
            "            logging.info(f\"BM25 search results: {bm25_results}\")\n",
            "            # Return (text, score, metadata)\n",
            "            return bm25_results\n",
            "        except Exception as e:\n",
            "            logging.error(f\"Whoosh search error: {e}\")\n",
            "            return []\n",
            "\n",
            "def combine_results(vector_results, bm25_results, alpha: float = 0.5):\n",
            "    \"\"\"\n",
            "    Combines FAISS and Whoosh results using Reciprocal Rank Fusion.\n",
            "\n",
            "    Args:\n",
            "        vector_results (list): List of tuples (text, score, metadata) from FAISS.\n",
            "        bm25_results (list): List of tuples (text, score, metadata) from Whoosh.\n",
            "        alpha (float, optional): Weight for FAISS results. Defaults to 0.5.\n",
            "\n",
            "    Returns:\n",
            "        list: Combined list of tuples (text, combined_score, metadata).\n",
            "    \"\"\"\n",
            "    combined_scores = {}\n",
            "    metadata_mapping = {}\n",
            "\n",
            "    # Process FAISS results\n",
            "    for idx, (text, score, metadata) in enumerate(vector_results):\n",
            "        if text:\n",
            "            combined_scores[text] = combined_scores.get(text, 0) + alpha / (idx + 1)\n",
            "            metadata_mapping[text] = metadata  # Store metadata from FAISS\n",
            "\n",
            "    # Process BM25 results\n",
            "    for idx, (text, score, metadata) in enumerate(bm25_results):\n",
            "        if text:\n",
            "            combined_scores[text] = combined_scores.get(text, 0) + (1 - alpha) / (idx + 1)\n",
            "            if text not in metadata_mapping:\n",
            "                metadata_mapping[text] = metadata  # Store metadata from Whoosh if not already present\n",
            "\n",
            "    # Combine scores and retrieve metadata\n",
            "    combined_results = [(text, score, metadata_mapping[text]) for text, score in combined_scores.items()]\n",
            "\n",
            "    # Sort results by combined score in descending order\n",
            "    sorted_results = sorted(combined_results, key=lambda x: x[1], reverse=True)\n",
            "    return sorted_results\n",
            "\n",
            "\n",
            "import numpy as np\n",
            "\n",
            "# Load a pretrained cross-encoder model\n",
            "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-TinyBERT-L-6\")\n",
            "\n",
            "def rerank_with_cross_encoder(query: str, documents: list, top_k: int = 10, batch_size: int = 16):\n",
            "    \"\"\"\n",
            "    Rerank documents using a cross-encoder model with batching.\n",
            "\n",
            "    Args:\n",
            "        query (str): The user's query.\n",
            "        documents (list): List of tuples (text, combined_score, metadata).\n",
            "        top_k (int, optional): Number of top documents to return after reranking. Defaults to 10.\n",
            "        batch_size (int, optional): Number of document-query pairs to process in a batch. Defaults to 16.\n",
            "\n",
            "    Returns:\n",
            "        list: Reranked list of top-k documents with their scores and metadata.\n",
            "    \"\"\"\n",
            "    try:\n",
            "        # Prepare input pairs for the cross-encoder\n",
            "        input_pairs = [(query, doc[0]) for doc in documents]  # doc[0] is the document text\n",
            "        scores = []\n",
            "\n",
            "        # Process input pairs in batches\n",
            "        for i in range(0, len(input_pairs), batch_size):\n",
            "            batch = input_pairs[i:i + batch_size]\n",
            "            try:\n",
            "                batch_scores = cross_encoder.predict(batch)  # Predict scores for the batch\n",
            "                scores.extend(batch_scores)\n",
            "            except Exception as e:\n",
            "                logging.error(f\"Cross-encoder prediction failed for batch {i // batch_size}: {e}\")\n",
            "                scores.extend([0.0] * len(batch))  # Fallback to zero scores for this batch\n",
            "\n",
            "        # Combine scores with documents\n",
            "        scored_docs = [(doc[0], score, doc[2]) for doc, score in zip(documents, scores)]\n",
            "\n",
            "        # Sort documents by relevance score in descending order\n",
            "        reranked_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)\n",
            "\n",
            "        # Filter by relevance threshold\n",
            "        filtered_reranked_docs = [doc for doc in reranked_docs if doc[1] >= reranking_treshold]\n",
            "\n",
            "        return filtered_reranked_docs[:top_k]  # Return documents from top-k, filtered by threshold\n",
            "\n",
            "    except Exception as e:\n",
            "        logging.error(f\"Unexpected error in rerank_with_cross_encoder: {e}\")\n",
            "        return []  # Return an empty list in case of failure\n",
            "\n",
            "\n",
            "\n",
            "# 6 Create a LLM-based Retriever based on Hybrid Search Logic\n",
            "\n",
            "import time  # Import time for timestamping\n",
            "\n",
            "def generate_llm_response(prompt: str) -> str:\n",
            "    \"\"\"\n",
            "    Generates a response from the LLM based on the given prompt.\n",
            "\n",
            "    Args:\n",
            "        prompt (str): The input prompt for the LLM.\n",
            "\n",
            "    Returns:\n",
            "        str: The response generated by the LLM.\n",
            "    \"\"\"\n",
            "    try:\n",
            "        response = llm.complete(prompt=prompt)\n",
            "        return response.text.strip()\n",
            "    except Exception as e:\n",
            "        logging.error(f\"LLM generation failed: {e}\")\n",
            "        return \"An error occurred while generating the response. Please try again later.\"\n",
            "\n",
            "def get_answer(query: str, top_k: int = 10, alpha: float = 0.5, history=None):\n",
            "    \"\"\"\n",
            "    Retrieves answers for a given query using FAISS, Whoosh indices, and a Cross-Encoder Reranker.\n",
            "    Enhances the references with metadata attributes from both FAISS and Whoosh.\n",
            "\n",
            "    Logs the time taken for each significant step.\n",
            "    \"\"\"\n",
            "    try:\n",
            "        start_time = time.time()  # Start timing\n",
            "        logging.info(\"Pipeline started.\")\n",
            "\n",
            "        if not query.strip():\n",
            "            logging.warning(\"Received an empty query.\")\n",
            "            return history, \"Please enter a valid question.\"\n",
            "\n",
            "        if history is None:\n",
            "            history = []\n",
            "\n",
            "        # Step 1: Perform FAISS Vector Search\n",
            "        vector_search_start = time.time()\n",
            "        vector_results = perform_vector_search(query, top_k * 2)\n",
            "        vector_search_time = time.time() - vector_search_start\n",
            "        logging.info(f\"Vector search completed in {vector_search_time:.2f} seconds.\")\n",
            "\n",
            "        # Step 2: Perform BM25 Search\n",
            "        bm25_search_start = time.time()\n",
            "        bm25_results = perform_bm25_search(query, top_k * 2)\n",
            "        bm25_search_time = time.time() - bm25_search_start\n",
            "        logging.info(f\"BM25 search completed in {bm25_search_time:.2f} seconds.\")\n",
            "\n",
            "        # Step 3: Combine Results\n",
            "        combine_results_start = time.time()\n",
            "        hybrid_results = combine_results(vector_results, bm25_results, alpha)\n",
            "        combine_results_time = time.time() - combine_results_start\n",
            "        logging.info(f\"Combining results completed in {combine_results_time:.2f} seconds.\")\n",
            "\n",
            "        # Step 4: Rerank with Cross-Encoder\n",
            "        rerank_start = time.time()\n",
            "        reranked_results = rerank_with_cross_encoder(query, hybrid_results, top_k=top_k, batch_size=top_k)  # Batch_size is also equal top_k\n",
            "        rerank_time = time.time() - rerank_start\n",
            "        logging.info(f\"Reranking completed in {rerank_time:.2f} seconds.\")\n",
            "\n",
            "        # Step 5: Prepare Context for LLM\n",
            "        context_start = time.time()\n",
            "        context = \"\\n\\n\".join([text for text, score, metadata in reranked_results])\n",
            "        context_time = time.time() - context_start\n",
            "        logging.info(f\"Context preparation completed in {context_time:.2f} seconds.\")\n",
            "\n",
            "        # Step 6: Generate LLM Response\n",
            "        llm_start = time.time()\n",
            "        llm_prompt = (\n",
            "            f\"Question: {query}\\n\\n\"\n",
            "            f\"Context:\\n{context}\\n\\n\"\n",
            "            \"Based on the provided context, is there enough relevant information to answer the question? \"\n",
            "            \"Respond with 'Yes' or 'No'. If yes, provide the answer; otherwise, say 'No relevant information available.'\"\n",
            "        )\n",
            "        llm_response = generate_llm_response(llm_prompt)\n",
            "        llm_time = time.time() - llm_start\n",
            "        logging.info(f\"LLM response generated in {llm_time:.2f} seconds.\")\n",
            "\n",
            "        # Step 7: Format Final Response\n",
            "        response_format_start = time.time()\n",
            "        references = []\n",
            "        for text, score, metadata in reranked_results:\n",
            "            if metadata:\n",
            "                source = metadata.get(\"source\", \"Unknown Source\")\n",
            "                sheet = f\"Sheet: {metadata['sheet']}\" if metadata.get(\"sheet\") else \"\"\n",
            "                row = f\"Row: {metadata['row_number']}\" if metadata.get(\"row_number\") else \"\"\n",
            "                slide = f\"Slide: {metadata['slide_number']}\" if metadata.get(\"slide_number\") else \"\"\n",
            "                section = f\"Section: {metadata['section']}\" if metadata.get(\"section\") else \"\"\n",
            "                page = f\"Page: {metadata['page_number']}\" if metadata.get(\"page_number\") else \"\"\n",
            "                chunk = f\"Chunk: {metadata['chunk_number']}\" if metadata.get(\"chunk_number\") else \"\"\n",
            "                total_chunks = f\"Total Chunks: {metadata['total_chunks_in_section']}\" if metadata.get(\"total_chunks_in_section\") else \"\"\n",
            "\n",
            "                # Combine all available metadata\n",
            "                metadata_str = \", \".join(filter(None, [source, sheet, row, slide, section, page, chunk, total_chunks]))\n",
            "                references.append(f\"{metadata_str}: {text[:50]}...\")\n",
            "            else:\n",
            "                references.append(f\"{text[:50]}...\")\n",
            "\n",
            "        references_text = \"\\n\".join([f\"- {ref}\" for ref in references])\n",
            "        if llm_response == 'No relevant information available.':\n",
            "            final_answer = llm_response\n",
            "        else:\n",
            "            final_answer = f\"{llm_response}\\n\\nSources:\\n{references_text}\"\n",
            "\n",
            "        response_format_time = time.time() - response_format_start\n",
            "        logging.info(f\"Response formatting completed in {response_format_time:.2f} seconds.\")\n",
            "\n",
            "        # Update chat history\n",
            "        history.append((query, final_answer))\n",
            "        total_time = time.time() - start_time\n",
            "        logging.info(f\"Pipeline completed in {total_time:.2f} seconds.\")\n",
            "\n",
            "        return history, history\n",
            "\n",
            "    except Exception as e:\n",
            "        logging.error(f\"Unexpected Error in get_answer: {e}\")\n",
            "        response = \"An unexpected error occurred. Please try again later.\"\n",
            "        if history is not None:\n",
            "            history.append((query, response))\n",
            "        return history, history\n",
            "\n",
            "\n",
            "# 7. Build Gradio Interface\n",
            "\n",
            "import gradio as gr\n",
            "\n",
            "def chatbot_interface(user_input, history):\n",
            "    \"\"\"\n",
            "    Handles the Gradio interface interaction by invoking the get_answer function.\n",
            "\n",
            "    Args:\n",
            "        user_input (str): The user's input query.\n",
            "        history (list): The chat history (state).\n",
            "\n",
            "    Returns:\n",
            "        tuple: Updated history and the LLM response.\n",
            "    \"\"\"\n",
            "    try:\n",
            "        logging.info(f\"Received user input: {user_input}\")\n",
            "        if not user_input.strip():\n",
            "            return history, \"Please enter a valid query.\"\n",
            "\n",
            "        # Call the get_answer function\n",
            "        updated_history, response = get_answer(user_input, history=history)\n",
            "\n",
            "        # Log the response for debugging\n",
            "        logging.info(f\"LLM Response: {response}\")\n",
            "        return updated_history, response\n",
            "\n",
            "    except Exception as e:\n",
            "        logging.error(f\"Error in chatbot_interface: {e}\")\n",
            "        return history, \"An unexpected error occurred. Please try again later.\"\n",
            "\n",
            "# Initialize Gradio Blocks\n",
            "with gr.Blocks(css=\"\"\"\n",
            "    #user-input {\n",
            "        padding-top: 2px !important;\n",
            "        font-size: 24px !important;\n",
            "        line-height: 1.2 !important;\n",
            "        background-color: #f9f9f9 !important;\n",
            "    } \"\"\") as demo:\n",
            "\n",
            "    gr.Markdown(\"### RAG Chatbot for Oil & Gas Drilling Engineers\")\n",
            "\n",
            "    # Chatbot Tab\n",
            "    with gr.Tab(\"Chatbot\"):\n",
            "        with gr.Row():\n",
            "            chatbot = gr.Chatbot(label=\"Chat Window\", show_label=True)\n",
            "        with gr.Row(equal_height=True):\n",
            "            user_input = gr.Textbox(\n",
            "                lines=2,\n",
            "                placeholder=\"Enter your question here...\",\n",
            "                label=\"Your Message\",\n",
            "                elem_id=\"user-input\"\n",
            "            )\n",
            "            send_button = gr.Button(\"Send\", scale=0.25)\n",
            "        state = gr.State(value=[])  # Initialize state as an empty list\n",
            "\n",
            "        # Define interaction logic\n",
            "        send_button.click(\n",
            "            fn=chatbot_interface,\n",
            "            inputs=[user_input, state],\n",
            "            outputs=[chatbot, state]\n",
            "        )\n",
            "\n",
            "    # Settings Tab (Future Enhancements Placeholder)\n",
            "    with gr.Tab(\"Settings\"):\n",
            "        gr.Markdown(\"### Settings\")\n",
            "        gr.Markdown(\"Settings can be configured here in the future.\")\n",
            "\n",
            "    # Footer\n",
            "    gr.Markdown(\"© Noname Company\")\n",
            "\n",
            "# Launch Gradio interface\n",
            "demo.launch()\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5c5b410b01dd434284ffa4c10171bc2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_852f73b1aff14d8f8c4ade66af1dfa7d",
              "IPY_MODEL_0bb25efedcd3493bacfa530c80e48586",
              "IPY_MODEL_64d4a5d70e034825b888dc7f6b33dd6b"
            ],
            "layout": "IPY_MODEL_47a82eada50d4b67b477148ddf037708"
          }
        },
        "852f73b1aff14d8f8c4ade66af1dfa7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d22c7e1a3b649889f22d3fabeee635b",
            "placeholder": "​",
            "style": "IPY_MODEL_6d266f77df4e48d097daeb93ef54e5fa",
            "value": "config.json: 100%"
          }
        },
        "0bb25efedcd3493bacfa530c80e48586": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe33edd9056c443892c33bbbd50a6b43",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2364ab7d4ae04c13b80956548c8e7a28",
            "value": 612
          }
        },
        "64d4a5d70e034825b888dc7f6b33dd6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b633679d7f5f476f93f50beb5b94587a",
            "placeholder": "​",
            "style": "IPY_MODEL_bc6c0664680541599c9e878a60f5f475",
            "value": " 612/612 [00:00&lt;00:00, 23.5kB/s]"
          }
        },
        "47a82eada50d4b67b477148ddf037708": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d22c7e1a3b649889f22d3fabeee635b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d266f77df4e48d097daeb93ef54e5fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe33edd9056c443892c33bbbd50a6b43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2364ab7d4ae04c13b80956548c8e7a28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b633679d7f5f476f93f50beb5b94587a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc6c0664680541599c9e878a60f5f475": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb1b724aabfb43e7bf7592f741799a13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3ec32bd869a44569891c802f321829e",
              "IPY_MODEL_966905f594234002852196f1a886239b",
              "IPY_MODEL_474e1a2f3ce54d1ca29457e6a18ffc84"
            ],
            "layout": "IPY_MODEL_70b1c5325ba648a0a094be4ef8ba0a26"
          }
        },
        "e3ec32bd869a44569891c802f321829e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8ceb1fa29f94200be0629f948c1c75b",
            "placeholder": "​",
            "style": "IPY_MODEL_48a0e4ba5fc240088ea4c613e3d7587e",
            "value": "model.safetensors: 100%"
          }
        },
        "966905f594234002852196f1a886239b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_818910630c0a4ce793a64addf4a7b875",
            "max": 267839500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_40c8531279e44c669f90683671f6b700",
            "value": 267839500
          }
        },
        "474e1a2f3ce54d1ca29457e6a18ffc84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d8122f85b0245cca568bee2ca629979",
            "placeholder": "​",
            "style": "IPY_MODEL_b41c54effdaf45769a86e23624ae5d89",
            "value": " 268M/268M [00:01&lt;00:00, 183MB/s]"
          }
        },
        "70b1c5325ba648a0a094be4ef8ba0a26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8ceb1fa29f94200be0629f948c1c75b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48a0e4ba5fc240088ea4c613e3d7587e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "818910630c0a4ce793a64addf4a7b875": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40c8531279e44c669f90683671f6b700": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1d8122f85b0245cca568bee2ca629979": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b41c54effdaf45769a86e23624ae5d89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e96a9b5637bd42babe82ad869a27af1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_66319c1604f84a1e9699c81b9641968f",
              "IPY_MODEL_c54d395c2bf949bfaddb33c0e0662434",
              "IPY_MODEL_6d65720983c94e6b9a3790004ae7f88f"
            ],
            "layout": "IPY_MODEL_1a369e552b154b148c4c94fc52fd64f7"
          }
        },
        "66319c1604f84a1e9699c81b9641968f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fdd8c8b56594051a7414a3ac7955952",
            "placeholder": "​",
            "style": "IPY_MODEL_bf86487270c0437d838f639b1e985193",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "c54d395c2bf949bfaddb33c0e0662434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f81dd31b7d364bf79ccb81e53a5af9aa",
            "max": 541,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78ea5891bf724e1192302822e09e38d2",
            "value": 541
          }
        },
        "6d65720983c94e6b9a3790004ae7f88f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5567201bef6c42fea4c71684367d9a48",
            "placeholder": "​",
            "style": "IPY_MODEL_dd771157bcb5420db68dba5da011a7f3",
            "value": " 541/541 [00:00&lt;00:00, 24.6kB/s]"
          }
        },
        "1a369e552b154b148c4c94fc52fd64f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fdd8c8b56594051a7414a3ac7955952": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf86487270c0437d838f639b1e985193": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f81dd31b7d364bf79ccb81e53a5af9aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78ea5891bf724e1192302822e09e38d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5567201bef6c42fea4c71684367d9a48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd771157bcb5420db68dba5da011a7f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1b8e23ce7c0463e80e2f76236e8c70e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e5d70e6934e4833a61a71b84760f1f0",
              "IPY_MODEL_584ed65d62af43898f24eef4a18a936f",
              "IPY_MODEL_76b7b94516ec46aea09b40593c22c491"
            ],
            "layout": "IPY_MODEL_36acd112da7a4b3b83bd83f8aa60f099"
          }
        },
        "9e5d70e6934e4833a61a71b84760f1f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6718465be6e344d3a2ce3e48dbddcf6e",
            "placeholder": "​",
            "style": "IPY_MODEL_9e79e7dd68c84d0b91717dca58a46b4a",
            "value": "vocab.txt: 100%"
          }
        },
        "584ed65d62af43898f24eef4a18a936f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06568652bb334757b25392001cf85d68",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_44dadd41b1c54d398e289d935e780974",
            "value": 231508
          }
        },
        "76b7b94516ec46aea09b40593c22c491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe80e5b382b6424baf7662ebcbeb8b06",
            "placeholder": "​",
            "style": "IPY_MODEL_ad105995fbf74658a1a5c6ef92f7aece",
            "value": " 232k/232k [00:00&lt;00:00, 3.40MB/s]"
          }
        },
        "36acd112da7a4b3b83bd83f8aa60f099": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6718465be6e344d3a2ce3e48dbddcf6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e79e7dd68c84d0b91717dca58a46b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06568652bb334757b25392001cf85d68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44dadd41b1c54d398e289d935e780974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe80e5b382b6424baf7662ebcbeb8b06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad105995fbf74658a1a5c6ef92f7aece": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31c40bf81840437fbd16c924e441a26a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9bad7a2a4dec4e5da12be0479f5d10bf",
              "IPY_MODEL_c044251d70bb4bd986c1fb4e8406166f",
              "IPY_MODEL_1ca72ef741df4da1a86978e1c500bef8"
            ],
            "layout": "IPY_MODEL_c4b6d97fc7e748bdbcdf524cab3158f6"
          }
        },
        "9bad7a2a4dec4e5da12be0479f5d10bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4aaea26fe4dc4fe9a4594e1e46b05db8",
            "placeholder": "​",
            "style": "IPY_MODEL_72ad734a58a44a5c9a2ed29a05b92832",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "c044251d70bb4bd986c1fb4e8406166f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ee46df6f5444d059b23c5b0651fec6f",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_10950058258043ad9b76fe87ca751317",
            "value": 112
          }
        },
        "1ca72ef741df4da1a86978e1c500bef8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79a03fcd048042598f97d3c63adffe0a",
            "placeholder": "​",
            "style": "IPY_MODEL_435cb06c587f472f9eb99ddb65b2e8a0",
            "value": " 112/112 [00:00&lt;00:00, 4.82kB/s]"
          }
        },
        "c4b6d97fc7e748bdbcdf524cab3158f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4aaea26fe4dc4fe9a4594e1e46b05db8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72ad734a58a44a5c9a2ed29a05b92832": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ee46df6f5444d059b23c5b0651fec6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10950058258043ad9b76fe87ca751317": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79a03fcd048042598f97d3c63adffe0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "435cb06c587f472f9eb99ddb65b2e8a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}